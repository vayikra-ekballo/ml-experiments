{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7a6e12c3122d4adb9f5d9fac4c50aafe":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_924b879971994e519993d9acc49aff39","IPY_MODEL_d480ee3b185f475b8f4d9a95a86d421e","IPY_MODEL_a12c3ba3508f4e819ab3a4fbbb78f867"],"layout":"IPY_MODEL_d2d2b294150e4ea583c1170540947fa4"}},"924b879971994e519993d9acc49aff39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da092b4081a946ee93261cbaa7fd2887","placeholder":"​","style":"IPY_MODEL_368b440be05e49509a419f499ecfdedc","value":"Map: 100%"}},"d480ee3b185f475b8f4d9a95a86d421e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_926bec3c022a49688a0c3552e1180a55","max":30686,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c060443bbc8d4e8abc3a4af416d3635b","value":30686}},"a12c3ba3508f4e819ab3a4fbbb78f867":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_188aab683f0042cd898bfd44e77b777c","placeholder":"​","style":"IPY_MODEL_264f859e489f447db5d1a7aeb3bbeae0","value":" 30686/30686 [00:03&lt;00:00, 7793.48 examples/s]"}},"d2d2b294150e4ea583c1170540947fa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da092b4081a946ee93261cbaa7fd2887":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"368b440be05e49509a419f499ecfdedc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"926bec3c022a49688a0c3552e1180a55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c060443bbc8d4e8abc3a4af416d3635b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"188aab683f0042cd898bfd44e77b777c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"264f859e489f447db5d1a7aeb3bbeae0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c97b6ec85c884a36906706c0ab0d5053":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cbd88d9db0764cf8b74ba14897d42ac5","IPY_MODEL_220b2259152240669f73c52fe957cfa5","IPY_MODEL_4977eeb26211419a9130ffdc48ed2024"],"layout":"IPY_MODEL_4191f22fd9ff415ba15f28b752ce648e"}},"cbd88d9db0764cf8b74ba14897d42ac5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8814945216643bc82eeae58977a9a67","placeholder":"​","style":"IPY_MODEL_2551854c103b406dbf7662956e0a3987","value":"Map: 100%"}},"220b2259152240669f73c52fe957cfa5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_89c5c7c842a64ff593f614c48749a2f8","max":4434,"min":0,"orientation":"horizontal","style":"IPY_MODEL_913c5574765b43b1aec597565a93fc97","value":4434}},"4977eeb26211419a9130ffdc48ed2024":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ff85b4bebfd438fba6287ca81a64689","placeholder":"​","style":"IPY_MODEL_a70462b250bd49109db39ea49dc1840d","value":" 4434/4434 [00:00&lt;00:00, 8133.58 examples/s]"}},"4191f22fd9ff415ba15f28b752ce648e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8814945216643bc82eeae58977a9a67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2551854c103b406dbf7662956e0a3987":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"89c5c7c842a64ff593f614c48749a2f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"913c5574765b43b1aec597565a93fc97":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6ff85b4bebfd438fba6287ca81a64689":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a70462b250bd49109db39ea49dc1840d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2dffce2337cb4ffa8e065775fcacfd2e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2810cbbfcae94e7e88495b6eadce7147","IPY_MODEL_dea3b0bcd7154a3ba2bd8d11c903e900","IPY_MODEL_64a00d72813a41ab888aed37c3f41055"],"layout":"IPY_MODEL_1b7b68c40ec1448aac2c581b9055043a"}},"2810cbbfcae94e7e88495b6eadce7147":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8855ce11434d400ea37243f41b0c6433","placeholder":"​","style":"IPY_MODEL_3e16fcc6212a46d0974539b9fe22baf4","value":"Map: 100%"}},"dea3b0bcd7154a3ba2bd8d11c903e900":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05ac1884d8724723956d0fc77782af85","max":4701,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3772bd63f8444ff8603f238efa5dcd8","value":4701}},"64a00d72813a41ab888aed37c3f41055":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2be46a2bba8e49599af2a04b646e8d6b","placeholder":"​","style":"IPY_MODEL_ce79b42630f04891a31a4dd049963944","value":" 4701/4701 [00:00&lt;00:00, 8597.01 examples/s]"}},"1b7b68c40ec1448aac2c581b9055043a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8855ce11434d400ea37243f41b0c6433":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e16fcc6212a46d0974539b9fe22baf4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05ac1884d8724723956d0fc77782af85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3772bd63f8444ff8603f238efa5dcd8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2be46a2bba8e49599af2a04b646e8d6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce79b42630f04891a31a4dd049963944":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1d47c68fbbaf411597b382bf597f392c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5f21275f742d4fe3a7d55348e93006e5","IPY_MODEL_b0be78a0f4e24009a69d290eeb0a94e0","IPY_MODEL_020c0cbaa9e544c191b3df6120ed24d5"],"layout":"IPY_MODEL_793d3437b03c4bdca437b8640835a8bf"}},"5f21275f742d4fe3a7d55348e93006e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7739a120ab634fd3850ecbb396a84e05","placeholder":"​","style":"IPY_MODEL_f2ae48f6cd5141bb928b3cf78b71838f","value":"Downloading builder script: "}},"b0be78a0f4e24009a69d290eeb0a94e0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_689306e99ba44808a7f5b9b0452f51c1","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c80ac526b98d411ba4b12443558b85b1","value":1}},"020c0cbaa9e544c191b3df6120ed24d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e802c6c6f385486cb225ff7527d9326b","placeholder":"​","style":"IPY_MODEL_96b19cc70c67423dab35809e5f6b546a","value":" 6.34k/? [00:00&lt;00:00, 694kB/s]"}},"793d3437b03c4bdca437b8640835a8bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7739a120ab634fd3850ecbb396a84e05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2ae48f6cd5141bb928b3cf78b71838f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"689306e99ba44808a7f5b9b0452f51c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"c80ac526b98d411ba4b12443558b85b1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e802c6c6f385486cb225ff7527d9326b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96b19cc70c67423dab35809e5f6b546a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75698bd51f314c9f817de00b21b86700":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ead22e6cf7464febb6fbbf7c7b76ae30","IPY_MODEL_cb4f1699037b4f2287862df02d6926cc","IPY_MODEL_bfd4052d35294eb988d2ac747874b2c2"],"layout":"IPY_MODEL_dbdbdd7c3ed4461bb167f178e33a837c"}},"ead22e6cf7464febb6fbbf7c7b76ae30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5ed98ef644f4e8ea1d4678e5b07ad28","placeholder":"​","style":"IPY_MODEL_dc31728d88524c9e8156934aa96c140e","value":"Map: 100%"}},"cb4f1699037b4f2287862df02d6926cc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b79b9a7ea044561a083210e9ae03dde","max":7198,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1ffe4ed3425f4affb1026c69572295a2","value":7198}},"bfd4052d35294eb988d2ac747874b2c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_896a86c67ec74d919760382197a3aae2","placeholder":"​","style":"IPY_MODEL_93c4ac7eb48f41ef8d332e3b63aa3f13","value":" 7198/7198 [00:02&lt;00:00, 3025.40 examples/s]"}},"dbdbdd7c3ed4461bb167f178e33a837c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ed98ef644f4e8ea1d4678e5b07ad28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc31728d88524c9e8156934aa96c140e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b79b9a7ea044561a083210e9ae03dde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ffe4ed3425f4affb1026c69572295a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"896a86c67ec74d919760382197a3aae2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93c4ac7eb48f41ef8d332e3b63aa3f13":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb71710c466b4bd8937f3c8cb5757076":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dfa6030874f747d0adda6f9996e248f8","IPY_MODEL_ac62c1bea41541da8b049998e4d54d8e","IPY_MODEL_b85bf8f81c864cd6bde2c4a1a7f2d071"],"layout":"IPY_MODEL_f755dbc6b52d492f920ec692a8e08773"}},"dfa6030874f747d0adda6f9996e248f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94f44a1ced794375b4d3bd98fb0b3959","placeholder":"​","style":"IPY_MODEL_6f79b0ac6f234868a23344ca3eb7edf1","value":"Map: 100%"}},"ac62c1bea41541da8b049998e4d54d8e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bd82499eec64394b2800d7a19a874e1","max":284,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dc2494259a9e4f8194c002d17b1a6022","value":284}},"b85bf8f81c864cd6bde2c4a1a7f2d071":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee43db8c594447969c371b979bfc6cd2","placeholder":"​","style":"IPY_MODEL_adfac0b11fb34f2ab78350e9264f13ae","value":" 284/284 [00:00&lt;00:00, 3387.59 examples/s]"}},"f755dbc6b52d492f920ec692a8e08773":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94f44a1ced794375b4d3bd98fb0b3959":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f79b0ac6f234868a23344ca3eb7edf1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9bd82499eec64394b2800d7a19a874e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc2494259a9e4f8194c002d17b1a6022":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee43db8c594447969c371b979bfc6cd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"adfac0b11fb34f2ab78350e9264f13ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ce39df886e24301b8767979cb15abd4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_142d0b64cd4343148b5b9e4d7272ee00","IPY_MODEL_4a84077d5d4a491eb1399206f048b5ef","IPY_MODEL_3350521b79e84a6a81307c8fee981376"],"layout":"IPY_MODEL_c42d406124794a9ea504957c1ae7f678"}},"142d0b64cd4343148b5b9e4d7272ee00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06c172c9668a4fda90fc6ae1f79a3262","placeholder":"​","style":"IPY_MODEL_c9eca4e968bb44e8a17cd803b2cbcbe9","value":"Map: 100%"}},"4a84077d5d4a491eb1399206f048b5ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_50b386cbdab5463a8521b1e214e8aacf","max":443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4cfd67597a5b4507a773919f0c637569","value":443}},"3350521b79e84a6a81307c8fee981376":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6d4e4bbcf294c9abcf6b6e16fb0aa40","placeholder":"​","style":"IPY_MODEL_70620786d7ee44aca8fb8c8ae2144e7d","value":" 443/443 [00:00&lt;00:00, 3301.83 examples/s]"}},"c42d406124794a9ea504957c1ae7f678":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06c172c9668a4fda90fc6ae1f79a3262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9eca4e968bb44e8a17cd803b2cbcbe9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50b386cbdab5463a8521b1e214e8aacf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cfd67597a5b4507a773919f0c637569":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6d4e4bbcf294c9abcf6b6e16fb0aa40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70620786d7ee44aca8fb8c8ae2144e7d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Language model applied to Ancient Greek\n","\n","Students:\n","<pre>Sarah Batara [qq577530]</pre>\n","<pre>Arjun Menon []</pre>\n","<pre>Javier Marsicano [qq577517]</pre>"],"metadata":{"id":"WsQD7ZVWKmKb"}},{"cell_type":"markdown","source":["## Motivation\n","We chose this topic because we were interested in language models and we had some prior experience with NLP as well. We found out that, for the majority of language tasks, LLMs achieve state-of-the-art performance. However, LLMs have challenges with some use cases, low-resource languages is one of them. As the name implies, datasets and corpora for low-resource languages are scarce or simply insufficient for training models that require large amounts of training data like LLMs. Currently there's still a lot of research and work going on to apply Deep Learning to low-resource languages, for instance, dialects spoken by specific ethnic groups in Africa or Asia.\n","\n","Thus, Ancient or classical languages are a good example of low-resource languages, Ancient Greek in particular. Furthermore, it's worth noting that this language has evolved significantly over the time, with changes in grammar, vocabulary, lexicon, etc. Specifically, Ancient Greek has undergone several distinct phases over a few centuries, including Classical Greek, Hellenistic Greek, and Koine Greek, each with its own unique characteristics. This evolution makes it even more challenging to create a single, comprehensive language model that can accurately represent the complexities of Ancient Greek.\n","\n","In particular, nowadays there's still little work on language models applied to Ancient Greek specifically, but we could find two interesting papers that were published fairly recently (two years ago). One is [Kevin Krahn, Derrick Tate, and Andrew C. Lamicela (2023) Sentence Embedding Models for Ancient Greek Using Multilingual Knowledge Distillation](https://aclanthology.org/2023.alp-1.2/) based on multilingual models; and the other is [Exploring Large Language Models for Classical Philology](https://aclanthology.org/2023.acl-long.846/) based on monolingual models. In their research, the authors leveraged a multilingual translation model to achieve high accuracy in translating Ancient Greek texts.\n","\n","While their results were promising, we identified several areas for potential improvement. In this project, we aim to explore a more targeted approach focusing specifically on Koine Greek, to determine whether a specialized model can yield better translation performance.\n","\n","##Model\n","\n","For low-resource languages BERT - or any of its variants - is the preferred model. There's a variant specially trained for [Ancient Greek](https://github.com/pranaydeeps/Ancient-Greek-BERT) which seems to be the first one published. Later on an even more elaborated language model was published, GreBERTa, which derives from RoBERTa, and is suitable to be fine-tuned to perform NLP tasks and trained specifically on Ancient Greek without using any modern Greek dataset.   \n","\n","##Datasets\n","\n","In our work, we plan to use the same datasets employed in these studies, along with several additional ones we have found published in repositories. The New Testament has been originally written in Ancient Greek (Koine) and it has been one of the most translated texts throughout history. Because of that, it has become a central reference point for translation theory and linguistics. Hence, almost all related work used the New Testament as part of the dataset since there's already plenty of repositories with the text already formatted and annotated. The most relevant ones are:\n","\n","https://github.com/STEPBible/STEPBible-Data\n","\n","https://github.com/Faithlife/SBLGNT\n","\n","https://github.com/OpenGreekAndLatin/First1KGreek\n","\n","https://github.com/proiel/proiel-treebank/\n","\n","\n","\n"],"metadata":{"id":"Z4VoyC4Oxqm9"}},{"cell_type":"code","source":["!pip install evaluate seqeval Dataset\n","!pip install datasets==3.6.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"xg87Ss8XEw-Q","executionInfo":{"status":"ok","timestamp":1763697128056,"user_tz":480,"elapsed":16332,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"0a37bbfb-ba27-4895-80e7-c29a58ab061d","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting evaluate\n","  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting Dataset\n","  Downloading dataset-1.6.2-py2.py3-none-any.whl.metadata (1.9 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n","Collecting sqlalchemy<2.0.0,>=1.3.2 (from Dataset)\n","  Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: alembic>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from Dataset) (1.17.1)\n","Collecting banal>=1.0.1 (from Dataset)\n","  Downloading banal-1.0.6-py2.py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=0.6.2->Dataset) (1.3.10)\n","Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=0.6.2->Dataset) (4.15.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2025.10.5)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<2.0.0,>=1.3.2->Dataset) (3.2.4)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=0.6.2->Dataset) (3.0.3)\n","Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n","Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n","Downloading SQLAlchemy-1.4.54-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=a1c8939d68b4d097390d471dc858a365b7d3c1d6f9df465301fcd5e9acaa232f\n","  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n","Successfully built seqeval\n","Installing collected packages: banal, sqlalchemy, seqeval, Dataset, evaluate\n","  Attempting uninstall: sqlalchemy\n","    Found existing installation: SQLAlchemy 2.0.44\n","    Uninstalling SQLAlchemy-2.0.44:\n","      Successfully uninstalled SQLAlchemy-2.0.44\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\n","google-adk 1.17.0 requires sqlalchemy<3.0.0,>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Dataset-1.6.2 banal-1.0.6 evaluate-0.4.6 seqeval-1.2.2 sqlalchemy-1.4.54\n","Collecting datasets==3.6.0\n","  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.20.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.32.4)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.70.16)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.36.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (6.0.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.13.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.2.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.10.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.22.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n","Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: datasets\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 4.0.0\n","    Uninstalling datasets-4.0.0:\n","      Successfully uninstalled datasets-4.0.0\n","Successfully installed datasets-3.6.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["datasets"]},"id":"74d4b879f56140bcb39ed59254cda2da"}},"metadata":{}}]},{"cell_type":"code","source":["#Import required libraries\n","import  pandas as pd\n","import csv\n","import re\n","import json\n","import argparse\n","from pathlib import Path\n","from collections import Counter\n","from typing import Dict, List, Tuple\n","import torch\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForTokenClassification,\n","    AutoModel,\n","    get_linear_schedule_with_warmup\n",")\n","from datasets import Dataset, DatasetDict\n","from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n","import random\n","from dataclasses import dataclass\n","from tqdm import tqdm\n","import os, warnings, unicodedata, numpy as np\n","import evaluate\n","from seqeval.metrics import classification_report\n","\n","try:\n","    import google.colab\n","\n","    IN_COLAB = True\n","except:\n","    IN_COLAB = False\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    path_prefix = '/content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/'\n","else:\n","    path_prefix = ''"],"metadata":{"id":"CwU5IuKXqTDI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763700358300,"user_tz":480,"elapsed":661,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"c440f895-97aa-4de3-f2de-d98ddf9ec8f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["##POS Tagging and Word Alignment"],"metadata":{"id":"aAXaHXMJ_Ocd"}},{"cell_type":"code","source":[],"metadata":{"id":"6FtoeqsDG1jE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fine-tuning GreBerta"],"metadata":{"id":"JjtVN8S2AOzc"}},{"cell_type":"code","source":["def load_pos_data(filepath: Path) -> List[Dict]:\n","    \"\"\"Load and extract POS tagging data from alignment JSON\"\"\"\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    pos_data = []\n","    for verse in data['data']:\n","        tokens = [t['word'] for t in verse['greek_tokens']]\n","        pos_tags = [t['pos'] for t in verse['greek_tokens']]\n","\n","        if tokens:  # Skip empty verses\n","            pos_data.append({\n","                'tokens': tokens,\n","                'pos_tags': pos_tags,\n","                'verse_id': verse['verse_id']\n","            })\n","\n","    return pos_data\n","\n","\n","def create_label_mapping(train_data: List[Dict]) -> Dict[str, int]:\n","    \"\"\"Create mapping from POS tags to integer IDs\"\"\"\n","    all_tags = set()\n","    for example in train_data:\n","        all_tags.update(example['pos_tags'])\n","\n","    # Sort for consistency\n","    sorted_tags = sorted(all_tags)\n","    tag_to_id = {tag: i for i, tag in enumerate(sorted_tags)}\n","    id_to_tag = {i: tag for tag, i in tag_to_id.items()}\n","\n","    return tag_to_id, id_to_tag\n","\n","\n","def tokenize_and_align_labels(examples, tokenizer, tag_to_id):\n","    \"\"\"\n","    Tokenize text and align POS labels with subword tokens.\n","\n","    When a word is split into subwords (e.g., 'λόγος' -> ['λ', '##όγος']),\n","    we assign the label to the first subword and -100 to the rest (ignored in loss).\n","    \"\"\"\n","    tokenized_inputs = tokenizer(\n","        examples['tokens'],\n","        truncation=True,\n","        is_split_into_words=True,\n","        padding=False,\n","        max_length=512\n","    )\n","\n","    labels = []\n","    for i, label_list in enumerate(examples['pos_tags']):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        label_ids = []\n","        previous_word_idx = None\n","\n","        for word_idx in word_ids:\n","            # Special tokens get -100 (ignored in loss)\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            # First subword of each word gets the label\n","            elif word_idx != previous_word_idx:\n","                label_ids.append(tag_to_id[label_list[word_idx]])\n","            # Other subwords get -100\n","            else:\n","                label_ids.append(-100)\n","\n","            previous_word_idx = word_idx\n","\n","        labels.append(label_ids)\n","\n","    tokenized_inputs['labels'] = labels\n","    return tokenized_inputs\n","\n","\n","def compute_metrics(eval_pred, id_to_tag):\n","    \"\"\"Compute accuracy and per-class metrics\"\"\"\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [id_to_tag[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [id_to_tag[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    # Flatten for metrics\n","    flat_predictions = [tag for sent in true_predictions for tag in sent]\n","    flat_labels = [tag for sent in true_labels for tag in sent]\n","\n","    accuracy = accuracy_score(flat_labels, flat_predictions)\n","\n","    return {\n","        'accuracy': accuracy,\n","        'num_examples': len(flat_labels)\n","    }\n","\n","\n","def train_pos_tagger():\n","    # Fine-tune GreBerta for POS tagging\n","    args = {\n","        'epochs': 3,  # Number of training epochs\n","        'batch_size': 16,  # Training batch size\n","        'lr': 2e-5,  # Learning rate\n","        'model_name': 'bowphs/GreBerta',  # Base model name\n","        'output_dir': path_prefix + 'pos_tagger_output', # Output directory\n","    }\n","\n","    print(\"=\" * 80)\n","    print(\"FINE-TUNING GREBERTA FOR POS TAGGING\")\n","    print(\"=\" * 80)\n","\n","    # 1. Load data\n","    print(\"\\n1. Loading data...\")\n","    train_data = load_pos_data(Path(path_prefix + 'data/train.json'))\n","    dev_data = load_pos_data(Path(path_prefix + 'data/dev.json'))\n","    test_data = load_pos_data(Path(path_prefix + 'data/test.json'))\n","\n","    print(f\"  Train: {len(train_data):,} verses\")\n","    print(f\"  Dev:   {len(dev_data):,} verses\")\n","    print(f\"  Test:  {len(test_data):,} verses\")\n","\n","    # 2. Create label mapping\n","    print(\"\\n2. Creating label mapping...\")\n","    tag_to_id, id_to_tag = create_label_mapping(train_data)\n","    num_labels = len(tag_to_id)\n","    print(f\"  Found {num_labels} POS tags:\")\n","    for tag, idx in sorted(tag_to_id.items(), key=lambda x: x[1]):\n","        # Count occurrences in train\n","        count = sum(1 for ex in train_data for t in ex['pos_tags'] if t == tag)\n","        print(f\"    {idx:2d}. {tag:5s} ({count:,} tokens)\")\n","\n","    # 3. Load tokenizer and model\n","    print(f\"\\n3. Loading {args['model_name']}...\")\n","    tokenizer = AutoTokenizer.from_pretrained(args['model_name'], add_prefix_space=True)\n","    model = AutoModelForTokenClassification.from_pretrained(\n","        args['model_name'],\n","        num_labels=num_labels,\n","        id2label=id_to_tag,\n","        label2id=tag_to_id\n","    )\n","    print(f\"  ✓ Model loaded with {num_labels} labels\")\n","    print(f\"  ✓ Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n","\n","    # 4. Create datasets\n","    print(\"\\n4. Creating Hugging Face datasets...\")\n","    train_dataset = Dataset.from_list(train_data)\n","\n","    dev_dataset = Dataset.from_list(dev_data)\n","    test_dataset = Dataset.from_list(test_data)\n","\n","\n","    # Tokenize\n","    print(\"  Tokenizing...\")\n","    train_dataset = train_dataset.map(\n","        lambda x: tokenize_and_align_labels(x, tokenizer, tag_to_id),\n","        batched=True,\n","        remove_columns=['tokens', 'pos_tags', 'verse_id']\n","    )\n","    dev_dataset = dev_dataset.map(\n","        lambda x: tokenize_and_align_labels(x, tokenizer, tag_to_id),\n","        batched=True,\n","        remove_columns=['tokens', 'pos_tags', 'verse_id']\n","    )\n","    test_dataset = test_dataset.map(\n","        lambda x: tokenize_and_align_labels(x, tokenizer, tag_to_id),\n","        batched=True,\n","        remove_columns=['tokens', 'pos_tags', 'verse_id']\n","    )\n","    print(f\"  ✓ Tokenized {len(train_dataset):,} training examples\")\n","\n","    # 5. Setup training\n","    print(f\"\\n5. Setting up training...\")\n","    print(f\"  Epochs: {args['epochs']}\")\n","    print(f\"  Batch size: {args['batch_size']}\")\n","    print(f\"  Learning rate: {args['lr']}\")\n","    print(f\"  Output dir: {args['output_dir']}\")\n","\n","    training_args = TrainingArguments(\n","        output_dir=args['output_dir'],\n","        learning_rate=args['lr'],\n","        per_device_train_batch_size=args['batch_size'],\n","        per_device_eval_batch_size=args['batch_size'],\n","        num_train_epochs=args['epochs'],\n","        weight_decay=0.01,\n","        eval_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        load_best_model_at_end=True,\n","        metric_for_best_model=\"accuracy\",\n","        push_to_hub=False,\n","        logging_dir=f'{args['output_dir']}/logs',\n","        logging_steps=50,\n","        report_to=\"none\" # Added to prevent W&B login prompt\n","    )\n","\n","    data_collator = DataCollatorForTokenClassification(tokenizer)\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=dev_dataset,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=lambda x: compute_metrics(x, id_to_tag),\n","    )\n","\n","    # 6. Train!\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"6. TRAINING STARTED\")\n","    print(\"=\" * 80)\n","\n","    train_result = trainer.train()\n","\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"TRAINING COMPLETE!\")\n","    print(\"=\" * 80)\n","    print(f\"\\nTraining metrics:\")\n","    for key, value in train_result.metrics.items():\n","        print(f\"  {key}: {value}\")\n","\n","    # 7. Evaluate on dev set\n","    print(\"\\n7. Evaluating on dev set...\")\n","    dev_results = trainer.evaluate(eval_dataset=dev_dataset)\n","    print(f\"Dev accuracy: {dev_results['eval_accuracy']:.4f}\")\n","\n","    # 8. Evaluate on test set\n","    print(\"\\n8. Evaluating on test set...\")\n","    test_results = trainer.evaluate(eval_dataset=test_dataset)\n","    print(f\"Test accuracy: {test_results['eval_accuracy']:.4f}\")\n","\n","    # 9. Save model\n","    print(f\"\\n9. Saving model to {args['output_dir']}...\")\n","    trainer.save_model(args['output_dir'])\n","    tokenizer.save_pretrained(args['output_dir'])\n","\n","    # Save label mappings\n","    import json\n","    with open(Path(args['output_dir']) / 'label_mapping.json', 'w') as f:\n","        json.dump({'tag_to_id': tag_to_id, 'id_to_tag': id_to_tag}, f, indent=2)\n","\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"✓ FINE-TUNING COMPLETE!\")\n","    print(\"=\" * 80)\n","    print(f\"\\nModel saved to: {args['output_dir']}\")\n","    print(f\"Dev accuracy:   {dev_results['eval_accuracy']:.4f}\")\n","    print(f\"Test accuracy:  {test_results['eval_accuracy']:.4f}\")\n","    print(\"\\nTo use the model:\")\n","    print(f\"  from transformers import AutoModelForTokenClassification, AutoTokenizer\")\n","    print(f\"  model = AutoModelForTokenClassification.from_pretrained('{args['output_dir']}')\")\n","    print(f\"  tokenizer = AutoTokenizer.from_pretrained('{args['output_dir']}')\")\n","    print(\"=\" * 80)\n","\n","\n","train_pos_tagger()"],"metadata":{"id":"aNoBd6VEG1q-","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["75698bd51f314c9f817de00b21b86700","ead22e6cf7464febb6fbbf7c7b76ae30","cb4f1699037b4f2287862df02d6926cc","bfd4052d35294eb988d2ac747874b2c2","dbdbdd7c3ed4461bb167f178e33a837c","a5ed98ef644f4e8ea1d4678e5b07ad28","dc31728d88524c9e8156934aa96c140e","6b79b9a7ea044561a083210e9ae03dde","1ffe4ed3425f4affb1026c69572295a2","896a86c67ec74d919760382197a3aae2","93c4ac7eb48f41ef8d332e3b63aa3f13","bb71710c466b4bd8937f3c8cb5757076","dfa6030874f747d0adda6f9996e248f8","ac62c1bea41541da8b049998e4d54d8e","b85bf8f81c864cd6bde2c4a1a7f2d071","f755dbc6b52d492f920ec692a8e08773","94f44a1ced794375b4d3bd98fb0b3959","6f79b0ac6f234868a23344ca3eb7edf1","9bd82499eec64394b2800d7a19a874e1","dc2494259a9e4f8194c002d17b1a6022","ee43db8c594447969c371b979bfc6cd2","adfac0b11fb34f2ab78350e9264f13ae","4ce39df886e24301b8767979cb15abd4","142d0b64cd4343148b5b9e4d7272ee00","4a84077d5d4a491eb1399206f048b5ef","3350521b79e84a6a81307c8fee981376","c42d406124794a9ea504957c1ae7f678","06c172c9668a4fda90fc6ae1f79a3262","c9eca4e968bb44e8a17cd803b2cbcbe9","50b386cbdab5463a8521b1e214e8aacf","4cfd67597a5b4507a773919f0c637569","b6d4e4bbcf294c9abcf6b6e16fb0aa40","70620786d7ee44aca8fb8c8ae2144e7d"]},"outputId":"c5de7f17-a1af-4200-bc4a-1032b1c4454e","executionInfo":{"status":"ok","timestamp":1763711332243,"user_tz":480,"elapsed":125916,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","FINE-TUNING GREBERTA FOR POS TAGGING\n","================================================================================\n","\n","1. Loading data...\n","  Train: 7,198 verses\n","  Dev:   284 verses\n","  Test:  443 verses\n","\n","2. Creating label mapping...\n","  Found 13 POS tags:\n","     0. A-    (7,860 tokens)\n","     1. C-    (16,112 tokens)\n","     2. D-    (5,655 tokens)\n","     3. I-    (15 tokens)\n","     4. N-    (24,573 tokens)\n","     5. P-    (9,662 tokens)\n","     6. RA    (17,088 tokens)\n","     7. RD    (1,579 tokens)\n","     8. RI    (1,102 tokens)\n","     9. RP    (10,446 tokens)\n","    10. RR    (1,490 tokens)\n","    11. V-    (25,398 tokens)\n","    12. X-    (905 tokens)\n","\n","3. Loading bowphs/GreBerta...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at bowphs/GreBerta and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  ✓ Model loaded with 13 labels\n","  ✓ Model has 125,397,517 parameters\n","\n","4. Creating Hugging Face datasets...\n","  Tokenizing...\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/7198 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75698bd51f314c9f817de00b21b86700"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/284 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb71710c466b4bd8937f3c8cb5757076"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/443 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ce39df886e24301b8767979cb15abd4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["  ✓ Tokenized 7,198 training examples\n","\n","5. Setting up training...\n","  Epochs: 3\n","  Batch size: 16\n","  Learning rate: 2e-05\n","  Output dir: /content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/pos_tagger_output\n","\n","================================================================================\n","6. TRAINING STARTED\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2237812671.py:201: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1350/1350 01:54, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Num Examples</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.041300</td>\n","      <td>0.040670</td>\n","      <td>0.987398</td>\n","      <td>5158</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.023600</td>\n","      <td>0.033632</td>\n","      <td>0.989531</td>\n","      <td>5158</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.012400</td>\n","      <td>0.033520</td>\n","      <td>0.991276</td>\n","      <td>5158</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","================================================================================\n","TRAINING COMPLETE!\n","================================================================================\n","\n","Training metrics:\n","  train_runtime: 114.9723\n","  train_samples_per_second: 187.819\n","  train_steps_per_second: 11.742\n","  total_flos: 429776895059172.0\n","  train_loss: 0.09274685502052307\n","  epoch: 3.0\n","\n","7. Evaluating on dev set...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='46' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [18/18 00:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dev accuracy: 0.9913\n","\n","8. Evaluating on test set...\n","Test accuracy: 0.9932\n","\n","9. Saving model to /content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/pos_tagger_output...\n","\n","================================================================================\n","✓ FINE-TUNING COMPLETE!\n","================================================================================\n","\n","Model saved to: /content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/pos_tagger_output\n","Dev accuracy:   0.9913\n","Test accuracy:  0.9932\n","\n","To use the model:\n","  from transformers import AutoModelForTokenClassification, AutoTokenizer\n","  model = AutoModelForTokenClassification.from_pretrained('/content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/pos_tagger_output')\n","  tokenizer = AutoTokenizer.from_pretrained('/content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/pos_tagger_output')\n","================================================================================\n"]}]},{"cell_type":"markdown","source":["Word Alignment"],"metadata":{"id":"zyu-mHaSAHgG"}},{"cell_type":"code","source":["import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","@dataclass\n","class AlignmentExample:\n","    \"\"\"Single verse with alignment pairs\"\"\"\n","    verse_id: str\n","    greek_tokens: List[str]\n","    english_tokens: List[str]\n","    alignments: List[Tuple[int, int]]  # (greek_idx, english_idx) pairs\n","\n","\n","class AlignmentModel(nn.Module):\n","    \"\"\"Cross-lingual word alignment model\"\"\"\n","\n","    def __init__(self, greek_model_name='bowphs/GreBerta',\n","                 english_model_name='bert-base-uncased',\n","                 hidden_dim=256):\n","        super().__init__()\n","\n","        # Encoders\n","        self.greek_encoder = AutoModel.from_pretrained(greek_model_name)\n","        self.english_encoder = AutoModel.from_pretrained(english_model_name)\n","\n","        # Get hidden sizes\n","        greek_hidden = self.greek_encoder.config.hidden_size\n","        english_hidden = self.english_encoder.config.hidden_size\n","\n","        # Classification head\n","        self.classifier = nn.Sequential(\n","            nn.Linear(greek_hidden + english_hidden, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(hidden_dim, 128),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(128, 2)  # Binary: aligned or not\n","        )\n","\n","    def forward(self, greek_input_ids, greek_attention_mask,\n","                english_input_ids, english_attention_mask,\n","                greek_indices, english_indices):\n","        \"\"\"\n","        Args:\n","            greek_input_ids: [batch_size, max_greek_len]\n","            greek_attention_mask: [batch_size, max_greek_len]\n","            english_input_ids: [batch_size, max_english_len]\n","            english_attention_mask: [batch_size, max_english_len]\n","            greek_indices: [batch_size, num_pairs] - which Greek token for each pair\n","            english_indices: [batch_size, num_pairs] - which English token for each pair\n","\n","        Returns:\n","            logits: [batch_size, num_pairs, 2] - alignment scores\n","        \"\"\"\n","        # Encode Greek\n","        greek_outputs = self.greek_encoder(\n","            input_ids=greek_input_ids,\n","            attention_mask=greek_attention_mask\n","        )\n","        greek_embeddings = greek_outputs.last_hidden_state  # [batch, greek_len, hidden]\n","\n","        # Encode English\n","        english_outputs = self.english_encoder(\n","            input_ids=english_input_ids,\n","            attention_mask=english_attention_mask\n","        )\n","        english_embeddings = english_outputs.last_hidden_state  # [batch, eng_len, hidden]\n","\n","        # Gather embeddings for specified pairs\n","        batch_size, num_pairs = greek_indices.shape\n","\n","        # Get Greek embeddings for each pair\n","        greek_pair_embeddings = torch.gather(\n","            greek_embeddings,\n","            dim=1,\n","            index=greek_indices.unsqueeze(-1).expand(-1, -1, greek_embeddings.size(-1))\n","        )  # [batch, num_pairs, greek_hidden]\n","\n","        # Get English embeddings for each pair\n","        english_pair_embeddings = torch.gather(\n","            english_embeddings,\n","            dim=1,\n","            index=english_indices.unsqueeze(-1).expand(-1, -1, english_embeddings.size(-1))\n","        )  # [batch, num_pairs, english_hidden]\n","\n","        # Concatenate embeddings\n","        combined = torch.cat([greek_pair_embeddings, english_pair_embeddings], dim=-1)\n","\n","        # Classify each pair\n","        logits = self.classifier(combined)  # [batch, num_pairs, 2]\n","\n","        return logits\n","\n","def _get_single_item(self, idx):\n","    # ← paste your entire original __getitem__ body here (without the debug print)\n","    example = self.examples[idx]\n","    # ... rest of your code exactly as before\n","\n","class AlignmentDataset(Dataset):\n","    \"\"\"Dataset for word alignment training\"\"\"\n","\n","    def __init__(self, examples: List[AlignmentExample],\n","                 greek_tokenizer, english_tokenizer,\n","                 max_pairs_per_verse=50):\n","        self.examples = examples\n","        self.greek_tokenizer = greek_tokenizer\n","        self.english_tokenizer = english_tokenizer\n","        self.max_pairs_per_verse = max_pairs_per_verse\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","\n","        if torch.is_tensor(idx):\n","          idx = idx.tolist()\n","        if isinstance(idx, list):\n","            return [self._get_single_item(i) for i in idx]\n","        return self._get_single_item(idx)\n","\n","    def _get_single_item(self, idx):\n","\n","        example = self.examples[idx]\n","\n","\n","        # Tokenize Greek (join with spaces)\n","        greek_text = ' '.join(example.greek_tokens)\n","        greek_encoded = self.greek_tokenizer(\n","            greek_text,\n","            truncation=True,\n","            max_length=512,\n","            return_tensors='pt'\n","        )\n","\n","        # Tokenize English\n","        english_text = ' '.join(example.english_tokens)\n","        english_encoded = self.english_tokenizer(\n","            english_text,\n","            truncation=True,\n","            max_length=512,\n","            return_tensors='pt'\n","        )\n","\n","        # Map original word indices to subword indices\n","        greek_word_to_token = self._get_word_to_token_map(\n","            example.greek_tokens, greek_encoded\n","        )\n","        english_word_to_token = self._get_word_to_token_map(\n","            example.english_tokens, english_encoded\n","        )\n","\n","        # Create training pairs\n","        # Positive examples: actual alignments\n","        positive_pairs = []\n","        for greek_idx, english_idx in example.alignments:\n","            if greek_idx in greek_word_to_token and english_idx in english_word_to_token:\n","                positive_pairs.append((\n","                    greek_word_to_token[greek_idx],\n","                    english_word_to_token[english_idx],\n","                    1  # label: aligned\n","                ))\n","\n","        # Negative examples: random non-aligned pairs\n","        num_negatives = min(len(positive_pairs) * 2, self.max_pairs_per_verse)\n","        negative_pairs = []\n","\n","        all_greek_indices = list(greek_word_to_token.values())\n","        all_english_indices = list(english_word_to_token.values())\n","\n","        # Create set of positive pairs for fast lookup\n","        positive_set = {(g, e) for g, e, _ in positive_pairs}\n","\n","        attempts = 0\n","        while len(negative_pairs) < num_negatives and attempts < num_negatives * 10:\n","            g_idx = random.choice(all_greek_indices)\n","            e_idx = random.choice(all_english_indices)\n","            if (g_idx, e_idx) not in positive_set:\n","                negative_pairs.append((g_idx, e_idx, 0))  # label: not aligned\n","            attempts += 1\n","\n","        # Combine and shuffle\n","        all_pairs = positive_pairs + negative_pairs\n","        random.shuffle(all_pairs)\n","\n","        # Limit total pairs\n","        all_pairs = all_pairs[:self.max_pairs_per_verse]\n","\n","        if not all_pairs:\n","            # Create dummy pair if no valid pairs\n","            all_pairs = [(1, 1, 0)]\n","\n","        greek_indices = torch.tensor([p[0] for p in all_pairs], dtype=torch.long)\n","        english_indices = torch.tensor([p[1] for p in all_pairs], dtype=torch.long)\n","        labels = torch.tensor([p[2] for p in all_pairs], dtype=torch.long)\n","\n","        return {\n","            'greek_input_ids': greek_encoded['input_ids'].squeeze(0),\n","            'greek_attention_mask': greek_encoded['attention_mask'].squeeze(0),\n","            'english_input_ids': english_encoded['input_ids'].squeeze(0),\n","            'english_attention_mask': english_encoded['attention_mask'].squeeze(0),\n","            'greek_indices': greek_indices,\n","            'english_indices': english_indices,\n","            'labels': labels,\n","            'verse_id': example.verse_id\n","        }\n","\n","    def _get_word_to_token_map(self, words, encoded):\n","        \"\"\"Map word indices to their first subword token index\"\"\"\n","        word_to_token = {}\n","        word_ids = encoded.word_ids()\n","\n","        for token_idx, word_idx in enumerate(word_ids):\n","            if word_idx is not None and word_idx not in word_to_token:\n","                word_to_token[word_idx] = token_idx\n","\n","        return word_to_token\n","\n","\n","def collate_fn(batch):\n","    \"\"\"Custom collate function for batching\"\"\"\n","    # Find max lengths\n","    max_greek_len = max(item['greek_input_ids'].size(0) for item in batch)\n","    max_english_len = max(item['english_input_ids'].size(0) for item in batch)\n","    max_pairs = max(item['labels'].size(0) for item in batch)\n","\n","    # Pad everything\n","    greek_input_ids = []\n","    greek_attention_mask = []\n","    english_input_ids = []\n","    english_attention_mask = []\n","    greek_indices = []\n","    english_indices = []\n","    labels = []\n","    verse_ids = []\n","\n","    for item in batch:\n","        # Pad Greek\n","        g_len = item['greek_input_ids'].size(0)\n","        greek_input_ids.append(\n","            torch.cat([item['greek_input_ids'],\n","                      torch.zeros(max_greek_len - g_len, dtype=torch.long)])\n","        )\n","        greek_attention_mask.append(\n","            torch.cat([item['greek_attention_mask'],\n","                      torch.zeros(max_greek_len - g_len, dtype=torch.long)])\n","        )\n","\n","        # Pad English\n","        e_len = item['english_input_ids'].size(0)\n","        english_input_ids.append(\n","            torch.cat([item['english_input_ids'],\n","                      torch.zeros(max_english_len - e_len, dtype=torch.long)])\n","        )\n","        english_attention_mask.append(\n","            torch.cat([item['english_attention_mask'],\n","                      torch.zeros(max_english_len - e_len, dtype=torch.long)])\n","        )\n","\n","        # Pad pairs\n","        num_pairs = item['labels'].size(0)\n","        greek_indices.append(\n","            torch.cat([item['greek_indices'],\n","                      torch.zeros(max_pairs - num_pairs, dtype=torch.long)])\n","        )\n","        english_indices.append(\n","            torch.cat([item['english_indices'],\n","                      torch.zeros(max_pairs - num_pairs, dtype=torch.long)])\n","        )\n","        labels.append(\n","            torch.cat([item['labels'],\n","                      torch.full((max_pairs - num_pairs,), -100, dtype=torch.long)])  # -100 = ignore\n","        )\n","\n","        verse_ids.append(item['verse_id'])\n","\n","    return {\n","        'greek_input_ids': torch.stack(greek_input_ids),\n","        'greek_attention_mask': torch.stack(greek_attention_mask),\n","        'english_input_ids': torch.stack(english_input_ids),\n","        'english_attention_mask': torch.stack(english_attention_mask),\n","        'greek_indices': torch.stack(greek_indices),\n","        'english_indices': torch.stack(english_indices),\n","        'labels': torch.stack(labels),\n","        'verse_ids': verse_ids\n","    }\n","\n","\n","def load_alignment_data(filepath: Path) -> List[AlignmentExample]:\n","    \"\"\"Load alignment data from JSON\"\"\"\n","    with open(filepath, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    examples = []\n","    for verse in data['data']:\n","        if not verse['alignments']:\n","            continue\n","\n","        greek_tokens = [t['word'] for t in verse['greek_tokens']]\n","        english_tokens = [t['word'] for t in verse['english_tokens']]\n","        alignments = [(a['greek_idx'], a['english_idx'])\n","                     for a in verse['alignments']]\n","\n","        examples.append(AlignmentExample(\n","            verse_id=verse['verse_id'],\n","            greek_tokens=greek_tokens,\n","            english_tokens=english_tokens,\n","            alignments=alignments\n","        ))\n","\n","    return examples\n","\n","\n","def train_epoch(model, dataloader, optimizer, scheduler, device):\n","    \"\"\"Train for one epoch\"\"\"\n","    model.train()\n","    total_loss = 0\n","    all_preds = []\n","    all_labels = []\n","\n","    progress_bar = tqdm(dataloader, desc='Training')\n","\n","    for batch in progress_bar:\n","        # Move to device\n","        greek_input_ids = batch['greek_input_ids'].to(device)\n","        greek_attention_mask = batch['greek_attention_mask'].to(device)\n","        english_input_ids = batch['english_input_ids'].to(device)\n","        english_attention_mask = batch['english_attention_mask'].to(device)\n","        greek_indices = batch['greek_indices'].to(device)\n","        english_indices = batch['english_indices'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        # Forward pass\n","        logits = model(\n","            greek_input_ids, greek_attention_mask,\n","            english_input_ids, english_attention_mask,\n","            greek_indices, english_indices\n","        )\n","\n","        # Compute loss (only on valid pairs)\n","        loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n","        loss = loss_fn(logits.view(-1, 2), labels.view(-1))\n","\n","        # Backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        scheduler.step()\n","\n","        # Track metrics\n","        total_loss += loss.item()\n","\n","        # Get predictions\n","        preds = torch.argmax(logits, dim=-1)\n","        valid_mask = labels != -100\n","        all_preds.extend(preds[valid_mask].cpu().numpy())\n","        all_labels.extend(labels[valid_mask].cpu().numpy())\n","\n","        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n","\n","    # Compute metrics\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average='binary'\n","    )\n","\n","    return {\n","        'loss': total_loss / len(dataloader),\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }\n","\n","\n","def evaluate(model, dataloader, device):\n","    \"\"\"Evaluate model\"\"\"\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc='Evaluating'):\n","            greek_input_ids = batch['greek_input_ids'].to(device)\n","            greek_attention_mask = batch['greek_attention_mask'].to(device)\n","            english_input_ids = batch['english_input_ids'].to(device)\n","            english_attention_mask = batch['english_attention_mask'].to(device)\n","            greek_indices = batch['greek_indices'].to(device)\n","            english_indices = batch['english_indices'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            logits = model(\n","                greek_input_ids, greek_attention_mask,\n","                english_input_ids, english_attention_mask,\n","                greek_indices, english_indices\n","            )\n","\n","            preds = torch.argmax(logits, dim=-1)\n","            valid_mask = labels != -100\n","            all_preds.extend(preds[valid_mask].cpu().numpy())\n","            all_labels.extend(labels[valid_mask].cpu().numpy())\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        all_labels, all_preds, average='binary'\n","    )\n","\n","    return {\n","        'precision': precision,\n","        'recall': recall,\n","        'f1': f1\n","    }\n","\n","\n","def train_alignment():\n","    args = {\n","        'epochs': 3,\n","        'batch_size': 8,\n","        'lr': 2e-5,\n","        'output_dir': path_prefix+'alignment_model_output_colab'\n","    }\n","\n","    print(\"=\" * 80)\n","    print(\"TRAINING WORD ALIGNMENT MODEL\")\n","    print(\"=\" * 80)\n","\n","    # Device\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"\\nUsing device: {device}\")\n","\n","    # Load data\n","    print(\"\\n1. Loading data...\")\n","    train_examples = load_alignment_data(Path(path_prefix+'data/train.json'))\n","    dev_examples = load_alignment_data(Path(path_prefix+'data/dev.json'))\n","    test_examples = load_alignment_data(Path(path_prefix+'data/test.json'))\n","\n","    print(f\"  Train: {len(train_examples):,} verses with alignments\")\n","    print(f\"  Dev:   {len(dev_examples):,} verses\")\n","    print(f\"  Test:  {len(test_examples):,} verses\")\n","\n","    # Load tokenizers\n","    print(\"\\n2. Loading tokenizers...\")\n","    greek_tokenizer = AutoTokenizer.from_pretrained('bowphs/GreBerta')\n","    english_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","    print(\"  ✓ Tokenizers loaded\")\n","\n","    # Create datasets\n","    print(\"\\n3. Creating datasets...\")\n","    train_dataset = AlignmentDataset(train_examples, greek_tokenizer, english_tokenizer)\n","    dev_dataset = AlignmentDataset(dev_examples, greek_tokenizer, english_tokenizer)\n","    test_dataset = AlignmentDataset(test_examples, greek_tokenizer, english_tokenizer)\n","\n","    train_dataloader = DataLoader(\n","        train_dataset, batch_size=args['batch_size'],\n","        shuffle=True, collate_fn=collate_fn, num_workers=0\n","    )\n","    dev_dataloader = DataLoader(\n","        dev_dataset, batch_size=args['batch_size'],\n","        shuffle=False, collate_fn=collate_fn, num_workers=0\n","    )\n","    test_dataloader = DataLoader(\n","        test_dataset, batch_size=args['batch_size'],\n","        shuffle=False, collate_fn=collate_fn, num_workers=0\n","    )\n","\n","    print(f\"  ✓ Created {len(train_dataloader):,} training batches\")\n","\n","    # Create model\n","    print(\"\\n4. Creating model...\")\n","    model = AlignmentModel()\n","    model = model.to(device)\n","\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"  ✓ Model created\")\n","    print(f\"  Total parameters: {total_params:,}\")\n","    print(f\"  Trainable parameters: {trainable_params:,}\")\n","\n","    # Setup training\n","    print(\"\\n5. Setting up training...\")\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=args['lr'])\n","    num_training_steps = len(train_dataloader) * args['epochs']\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=num_training_steps // 10,\n","        num_training_steps=num_training_steps\n","    )\n","\n","    print(f\"  Epochs: {args['epochs']}\")\n","    print(f\"  Batch size: {args['batch_size']}\")\n","    print(f\"  Learning rate: {args['lr']}\")\n","    print(f\"  Output dir: {args['output_dir']}\")\n","\n","    # Train!\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"6. TRAINING STARTED\")\n","    print(\"=\" * 80)\n","\n","    best_f1 = 0\n","    for epoch in range(args['epochs']):\n","        print(f\"\\nEpoch {epoch + 1}/{args['epochs']}\")\n","        print(\"-\" * 80)\n","\n","        # Train\n","        train_metrics = train_epoch(model, train_dataloader, optimizer, scheduler, device)\n","        print(f\"Train - Loss: {train_metrics['loss']:.4f}, \"\n","              f\"P: {train_metrics['precision']:.4f}, \"\n","              f\"R: {train_metrics['recall']:.4f}, \"\n","              f\"F1: {train_metrics['f1']:.4f}\")\n","\n","        # Evaluate\n","        dev_metrics = evaluate(model, dev_dataloader, device)\n","        print(f\"Dev   - P: {dev_metrics['precision']:.4f}, \"\n","              f\"R: {dev_metrics['recall']:.4f}, \"\n","              f\"F1: {dev_metrics['f1']:.4f}\")\n","\n","        # Save best model\n","        if dev_metrics['f1'] > best_f1:\n","            best_f1 = dev_metrics['f1']\n","            output_dir = Path(args['output_dir'])\n","            output_dir.mkdir(exist_ok=True)\n","            torch.save(model.state_dict(), output_dir / 'best_model.pt')\n","            print(f\"  ✓ Saved best model (F1: {best_f1:.4f})\")\n","\n","    # Final evaluation\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"7. FINAL EVALUATION\")\n","    print(\"=\" * 80)\n","\n","    # Load best model\n","    model.load_state_dict(torch.load(Path(args['output_dir']) / 'best_model.pt'))\n","\n","    print(\"\\nDev set:\")\n","    dev_metrics = evaluate(model, dev_dataloader, device)\n","    print(f\"  Precision: {dev_metrics['precision']:.4f}\")\n","    print(f\"  Recall:    {dev_metrics['recall']:.4f}\")\n","    print(f\"  F1 Score:  {dev_metrics['f1']:.4f}\")\n","\n","    print(\"\\nTest set:\")\n","    test_metrics = evaluate(model, test_dataloader, device)\n","    print(f\"  Precision: {test_metrics['precision']:.4f}\")\n","    print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n","    print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n","\n","    # Save final artifacts\n","    print(f\"\\n8. Saving model and tokenizers...\")\n","    output_dir = Path(args['output_dir'])\n","    output_dir.mkdir(exist_ok=True)\n","\n","    torch.save(model.state_dict(), output_dir / 'model.pt')\n","    greek_tokenizer.save_pretrained(output_dir / 'greek_tokenizer')\n","    english_tokenizer.save_pretrained(output_dir / 'english_tokenizer')\n","\n","    # Save config\n","    config = {\n","        'greek_model': 'bowphs/GreBerta',\n","        'english_model': 'bert-base-uncased',\n","        'dev_f1': dev_metrics['f1'],\n","        'test_f1': test_metrics['f1'],\n","    }\n","    with open(output_dir / 'config.json', 'w') as f:\n","        json.dump(config, f, indent=2)\n","\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"✓ TRAINING COMPLETE!\")\n","    print(\"=\" * 80)\n","    print(f\"\\nModel saved to: {args['output_dir']}\")\n","    print(f\"Test F1 Score: {test_metrics['f1']:.4f}\")\n","    print(\"\\nTo use the model, see test_alignment.py\")\n","    print(\"=\" * 80)\n","\n","\n","train_alignment()"],"metadata":{"id":"W-aZEqS3_-LW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763699955969,"user_tz":480,"elapsed":571017,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"d5b0146b-a88a-4cfa-dc65-9ac3bee3d468"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["================================================================================\n","TRAINING WORD ALIGNMENT MODEL\n","================================================================================\n","\n","Using device: cuda\n","\n","1. Loading data...\n","  Train: 5,846 verses with alignments\n","  Dev:   284 verses\n","  Test:  380 verses\n","\n","2. Loading tokenizers...\n","  ✓ Tokenizers loaded\n","\n","3. Creating datasets...\n","  ✓ Created 731 training batches\n","\n","4. Creating model...\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at bowphs/GreBerta and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["  ✓ Model created\n","  Total parameters: 235,886,978\n","  Trainable parameters: 235,886,978\n","\n","5. Setting up training...\n","  Epochs: 3\n","  Batch size: 8\n","  Learning rate: 2e-05\n","  Output dir: /content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/alignment_model_output_colab\n","\n","================================================================================\n","6. TRAINING STARTED\n","================================================================================\n","\n","Epoch 1/3\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 731/731 [02:46<00:00,  4.39it/s, loss=0.3591]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.4756, P: 0.7374, R: 0.5678, F1: 0.6415\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 36/36 [00:00<00:00, 39.09it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Dev   - P: 0.7717, R: 0.7666, F1: 0.7691\n","  ✓ Saved best model (F1: 0.7691)\n","\n","Epoch 2/3\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 731/731 [02:45<00:00,  4.41it/s, loss=0.2739]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.2988, P: 0.8158, R: 0.8457, F1: 0.8305\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 36/36 [00:00<00:00, 39.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Dev   - P: 0.8004, R: 0.8550, F1: 0.8268\n","  ✓ Saved best model (F1: 0.8268)\n","\n","Epoch 3/3\n","--------------------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 731/731 [02:45<00:00,  4.42it/s, loss=0.2892]\n"]},{"output_type":"stream","name":"stdout","text":["Train - Loss: 0.2557, P: 0.8450, R: 0.8797, F1: 0.8620\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 36/36 [00:00<00:00, 36.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Dev   - P: 0.8254, R: 0.8567, F1: 0.8408\n","  ✓ Saved best model (F1: 0.8408)\n","\n","================================================================================\n","7. FINAL EVALUATION\n","================================================================================\n","\n","Dev set:\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 36/36 [00:00<00:00, 37.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  Precision: 0.8272\n","  Recall:    0.8658\n","  F1 Score:  0.8461\n","\n","Test set:\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 48/48 [00:01<00:00, 30.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["  Precision: 0.8754\n","  Recall:    0.9137\n","  F1 Score:  0.8942\n","\n","8. Saving model and tokenizers...\n","\n","================================================================================\n","✓ TRAINING COMPLETE!\n","================================================================================\n","\n","Model saved to: /content/drive/MyDrive/UofT DL Term project/GreBerta-experiment-2/alignment_model_output_colab\n","Test F1 Score: 0.8942\n","\n","To use the model, see test_alignment.py\n","================================================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZJrNE-ZL_-W3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Named Entity Recognition (NER)"],"metadata":{"id":"laqg042BG2GB"}},{"cell_type":"markdown","source":["Another relevant application of large language models (LLMs) in linguistic tasks is Named Entity Recognition (NER). Recent work by Beersmans et al. (2024) demonstrates this by combining transformer-based models with domain-specific knowledge to identify individuals in Ancient Greek texts. Their study, “Gotta catch ’em all!: Retrieving people in Ancient Greek texts combining transformer models and domain knowledge,” was presented at the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024) and provides a strong example of how NLP techniques can be adapted for historical languages.\n","\n","The authors built a model for Ancient greek NER task with F1 score of 0.826, which is State-of-Art as of current. Since Ancient Greek is a low-resource, highly inflected ancient language with limited annotated corpora, unlike modern high-resource languages (e.g., English, where CoNLL-2003 NER F1 scores exceed 0.93), ancient languages suffer from data scarcity, orthographic variations (e.g., diacritics, dialects), and domain noise (e.g., fragmentary inscriptions or papyri). SOTA in this niche is typically in the 0.80–0.89 range for transformer-based models on similar tasks.\n","\n","We attempted to do hyperparameters tuning for a better performance as there were 2 hyperparameters not tested in the original paper - Warmup ratio and batch size. Both of these are sensitive to transformer tuning.\n","Large batch size reduces noise, resulting in better token representation.\n","\n","This is particulary useful for complex morphologically rich languages like Ancient Greek. However, smaller batches tend to act like regularization allowing for better generalization. Thus tuning of batch size is to find the balance between overfitting and better token representation.\n","\n","Early in training, weights are not yet adapted to the task. A high learning rate too soon can destroy useful pretrained knowledge. Tuning warmup would allow the model to gently adjust before training. This parameter works well with small datasets as too high of Learning Rate early on can overfit quickly.\n","\n","\n","Data augmentation is another potential approach to improving the model’s F1 score, but it is not practical for this project. Given limited resources available for Koine Greek, we would need multiple models to do the following:\n","\n","1.   Start with Koine greek - English translation pairs.\n","2.   Annotate the English translations of the Koine Greek sentences with existing English NER Labelling models such as dslim/bert-base-NER\n","3. Then align the annotated english tokens to the corresponding Koine Greek tokens using the alignment model above.\n","4. With this we will obtain infered NER labels for the Koine Greek tokens.\n","\n","However even after this automated pipeline, human verification would still be required to ensure label accuracy. Producing a dataset of few thousands Koine Greek tokens under these constraints would be extremely time-consuming and effectively not feasible within the scope of this project."],"metadata":{"id":"-2qU301mG2NB"}},{"cell_type":"markdown","source":["The code below will take 3 hours to run on T4 colab"],"metadata":{"id":"tqAXjHn8sXL3"}},{"cell_type":"code","source":["from datasets import Dataset as HFDataset, DatasetDict\n","\n","if IN_COLAB:\n","\n","    ner_path_prefix = '/content/drive/MyDrive/UofT DL Term project/NER Data/'\n","else:\n","    ner_path_prefix = './NER Data'\n","\n","\n","def read_conll(p: Path):\n","    \"\"\"\n","    Parse CoNLL with format:\n","        [line_id]  token  [POS]  NER\n","    Example:\n","        110089790\tβίβλος\tO\n","    Returns: {\"tokens\": [...], \"ner_tags\": [...]}\n","    \"\"\"\n","    sents, labs = [], []\n","    with p.open(encoding=\"utf-8\") as f:\n","        sent, lab = [], []\n","        for i, raw in enumerate(f, 1):\n","            line = raw.strip()\n","            if not line or line.startswith(\"#\"):\n","                if sent:\n","                    sents.append(sent)\n","                    labs.append(lab)\n","                    sent, lab = [], []\n","                continue\n","\n","            # Split on whitespace (handles tabs and spaces)\n","            parts = line.split()\n","            if len(parts) < 2:\n","                print(f\"Warning: Line {i} in {p.name} has <2 columns → SKIPPED\")\n","                print(f\"    → {line!r}\")\n","                continue\n","\n","            if len(parts) == 2:\n","                token = parts[0]\n","                ner   = parts[1]\n","            else:\n","                token = parts[1]   # skip ID\n","                ner   = parts[-1]  # last column is NER\n","\n","            sent.append(unicodedata.normalize(\"NFC\", token))\n","            lab.append(ner)\n","\n","        if sent:\n","            sents.append(sent)\n","            labs.append(lab)\n","\n","    print(f\"Loaded {len(sents)} sentences from {p.name}\")\n","    return {\"tokens\": sents, \"ner_tags\": labs}\n","\n","# load data\n","train_path = Path(ner_path_prefix + 'train.conll')\n","val_path   = Path(ner_path_prefix + 'val.conll')\n","test_path  = Path(ner_path_prefix + 'test.conll')\n","\n","raw = {\n","    \"train\": read_conll(train_path),\n","    \"validation\": read_conll(val_path),\n","    \"test\": read_conll(test_path),\n","}\n","data = DatasetDict({k: HFDataset.from_dict(v) for k, v in raw.items()})\n","\n","#Model name -------------------------------------------------------------\n","model_name = \"Marijke/AG_BERT_hypopt_NER\"\n","tokenizer  = AutoTokenizer.from_pretrained(model_name)\n","#------------------------------------------------------------------------\n","\n","all_labels = sorted({l for s in data[\"train\"][\"ner_tags\"] for l in s})\n","label2id   = {l: i for i, l in enumerate(all_labels)}\n","id2label   = {i: l for l, i in label2id.items()}\n","\n","#tokenise + align labels\n","def tokenise_align(example):\n","    tok = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n","    aligned = []\n","    for i, labs in enumerate(example[\"ner_tags\"]):\n","        word_ids = tok.word_ids(batch_index=i)\n","        prev = None\n","        ids  = []\n","        for wid in word_ids:\n","            if wid is None:\n","                ids.append(-100)\n","            elif wid != prev:\n","                ids.append(label2id[labs[wid]])\n","            else:\n","                ids.append(-100)               # sub-word → ignore\n","            prev = wid\n","        aligned.append(ids)\n","    tok[\"labels\"] = aligned\n","    return tok\n","\n","tokenised = data.map(tokenise_align, batched=True,\n","                     remove_columns=data[\"train\"].column_names)\n","\n","\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(all_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n",")\n","\n","collator = DataCollatorForTokenClassification(tokenizer)\n","\n","\n","#Compute metrics to evaluate model performance for NER task\n","def compute_metrics(p):\n","    preds, labels = p\n","    preds = np.argmax(preds, axis=2)\n","\n","    true_labels = []\n","    pred_labels = []\n","\n","    for prediction, label in zip(preds, labels):\n","        true_seq = [id2label[l] for l in label if l != -100]\n","        pred_seq = [id2label[pred] for pred, l in zip(prediction, label) if l != -100]\n","        if true_seq:  # Only add if not empty\n","            true_labels.append(true_seq)\n","            pred_labels.append(pred_seq)\n","\n","    if not true_labels:\n","        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n","\n","\n","    metric = evaluate.load(\"seqeval\")\n","    results = metric.compute(predictions=pred_labels, references=true_labels)\n","\n","    return {\n","      \"precision\": results[\"overall_precision\"],\n","      \"recall\": results['overall_recall'],\n","      \"f1\": results[\"overall_f1\"]\n","    }\n","\n"],"metadata":{"id":"MHflmcN4sWO7","colab":{"base_uri":"https://localhost:8080/","height":164,"referenced_widgets":["7a6e12c3122d4adb9f5d9fac4c50aafe","924b879971994e519993d9acc49aff39","d480ee3b185f475b8f4d9a95a86d421e","a12c3ba3508f4e819ab3a4fbbb78f867","d2d2b294150e4ea583c1170540947fa4","da092b4081a946ee93261cbaa7fd2887","368b440be05e49509a419f499ecfdedc","926bec3c022a49688a0c3552e1180a55","c060443bbc8d4e8abc3a4af416d3635b","188aab683f0042cd898bfd44e77b777c","264f859e489f447db5d1a7aeb3bbeae0","c97b6ec85c884a36906706c0ab0d5053","cbd88d9db0764cf8b74ba14897d42ac5","220b2259152240669f73c52fe957cfa5","4977eeb26211419a9130ffdc48ed2024","4191f22fd9ff415ba15f28b752ce648e","e8814945216643bc82eeae58977a9a67","2551854c103b406dbf7662956e0a3987","89c5c7c842a64ff593f614c48749a2f8","913c5574765b43b1aec597565a93fc97","6ff85b4bebfd438fba6287ca81a64689","a70462b250bd49109db39ea49dc1840d","2dffce2337cb4ffa8e065775fcacfd2e","2810cbbfcae94e7e88495b6eadce7147","dea3b0bcd7154a3ba2bd8d11c903e900","64a00d72813a41ab888aed37c3f41055","1b7b68c40ec1448aac2c581b9055043a","8855ce11434d400ea37243f41b0c6433","3e16fcc6212a46d0974539b9fe22baf4","05ac1884d8724723956d0fc77782af85","c3772bd63f8444ff8603f238efa5dcd8","2be46a2bba8e49599af2a04b646e8d6b","ce79b42630f04891a31a4dd049963944"]},"executionInfo":{"status":"ok","timestamp":1763700551435,"user_tz":480,"elapsed":9418,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"77fd15ef-32dc-44ed-bc10-040cadd3b622"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 30686 sentences from train.conll\n","Loaded 4434 sentences from val.conll\n","Loaded 4701 sentences from test.conll\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/30686 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a6e12c3122d4adb9f5d9fac4c50aafe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4434 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c97b6ec85c884a36906706c0ab0d5053"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4701 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dffce2337cb4ffa8e065775fcacfd2e"}},"metadata":{}}]},{"cell_type":"markdown","source":["Hyperparameter tuning was performed using Hyperopt. Although Hyperopt is less commonly used today, we chose it to maintain consistency with the methodology described in the referenced paper."],"metadata":{"id":"HNpHdBPcsnkh"}},{"cell_type":"code","source":["from datasets import load_dataset\n","import evaluate\n","from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, STATUS_FAIL\n","from hyperopt.early_stop import no_progress_loss\n","\n","# -------------------------------\n","# HYPEROPT SEARCH SPACE\n","# Include 2 new parameters that were not tried in the paper - batch size and warmup ratio\n","# -------------------------------\n","FIXED_LR = 6.040686648207059e-05\n","FIXED_WD = 0.01\n","FIXED_EPOCH = 3\n","\n","space = {\n","    \"batch_size\":    hp.choice(\"batch_size\", [8, 16, 32]),            # 3 options\n","    \"warmup_ratio\":  hp.choice(\"warmup_ratio\", [0.0, 0.06, 0.1, 0.2]), # 4 options\n","    \"seed\": 123 #for reproducibility\n","}\n","\n","# -------------------------------\n","# OBJECTIVE FUNCTION used by Hyperopt to test parameters\n","# -------------------------------\n","def objective(params):\n","  # we are keeping these 3 hyperparameters from the paper itself as they have found the optimal values for the Learning Rate, Weight Decay\n","  # and number of training epoch\n","\n","    try:\n","        batch_size = params[\"batch_size\"]\n","        warmup_ratio = params[\"warmup_ratio\"]\n","\n","        model_for_trial = AutoModelForTokenClassification.from_pretrained(\n","            model_name,\n","            num_labels=len(all_labels),\n","            id2label=id2label,\n","            label2id=label2id,\n","        )\n","\n","\n","        total_steps = int(len(tokenised[\"train\"]) / batch_size * FIXED_EPOCH)\n","        warmup_steps = int(total_steps * warmup_ratio)\n","\n","        training_args = TrainingArguments(\n","            output_dir=f\"./hyperopt_trial_{int(FIXED_EPOCH)}_{batch_size}_{FIXED_LR:.2e}\",\n","            num_train_epochs=FIXED_EPOCH,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size * 2,\n","            learning_rate=FIXED_LR,\n","            weight_decay=FIXED_WD,\n","            warmup_steps=warmup_steps,\n","            lr_scheduler_type=\"linear\",\n","            eval_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            logging_strategy=\"epoch\",\n","            load_best_model_at_end=True,\n","            metric_for_best_model=\"f1\",\n","            greater_is_better=True,\n","            report_to=\"none\",\n","            seed=params[\"seed\"],\n","            dataloader_num_workers=4,\n","            disable_tqdm=False,\n","        )\n","\n","        trainer = Trainer(\n","            model=model_for_trial,\n","            args=training_args,\n","            train_dataset=tokenised[\"train\"],\n","            eval_dataset=tokenised[\"validation\"],\n","            tokenizer=tokenizer,\n","            data_collator=collator,\n","            compute_metrics=compute_metrics,\n","        )\n","\n","        trainer.train()\n","        metrics = trainer.evaluate()\n","\n","        return {\n","            \"loss\": -metrics[\"eval_f1\"],\n","            \"status\": STATUS_OK,\n","            \"eval_f1\": metrics[\"eval_f1\"],\n","            \"params\": params,\n","        }\n","\n","    except Exception as e:\n","        print(f\"Trial failed: {e}\")\n","        return {\"loss\": 10.0, \"status\": STATUS_FAIL}\n","\n","# -------------------------------\n","# RUN HYPEROPT\n","# -------------------------------\n","trials = Trials()\n","\n","best = fmin(\n","    fn=objective,\n","    space=space,\n","    algo=tpe.suggest,\n","    max_evals=12,\n","    trials=trials,\n","    rstate=np.random.default_rng(42),\n","    show_progressbar=True,\n",")\n","\n","# -------------------------------\n","# PRINT BEST RESULT\n","# -------------------------------\n","best_trial = trials.best_trial\n","print(\"\\n\" + \"=\"*60)\n","print(\"Best Hyperparameters found:\")\n","print(\"=\"*60)\n","print(f\"Best eval micro F1 : {best_trial['result']['eval_f1']:.4f}\")\n","print(f\"Batch size         : {int(best_trial['result']['params']['batch_size'])}\")\n","print(f\"Warmup ratio       : {best_trial['result']['params']['warmup_ratio']}\")\n","print(\"=\"*60)"],"metadata":{"id":"q8QqQJxBsWbv","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1d47c68fbbaf411597b382bf597f392c","5f21275f742d4fe3a7d55348e93006e5","b0be78a0f4e24009a69d290eeb0a94e0","020c0cbaa9e544c191b3df6120ed24d5","793d3437b03c4bdca437b8640835a8bf","7739a120ab634fd3850ecbb396a84e05","f2ae48f6cd5141bb928b3cf78b71838f","689306e99ba44808a7f5b9b0452f51c1","c80ac526b98d411ba4b12443558b85b1","e802c6c6f385486cb225ff7527d9326b","96b19cc70c67423dab35809e5f6b546a"]},"executionInfo":{"status":"ok","timestamp":1763709712707,"user_tz":480,"elapsed":9114971,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"df8f3daa-abb5-4fb2-ed19-e9776ee07e8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\r  0%|          | 0/12 [00:00<?, ?trial/s, best loss=?]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.035100</td>\n","      <td>0.093696</td>\n","      <td>0.824993</td>\n","      <td>0.837818</td>\n","      <td>0.831356</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.020800</td>\n","      <td>0.102668</td>\n","      <td>0.822042</td>\n","      <td>0.850673</td>\n","      <td>0.836113</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.011300</td>\n","      <td>0.118540</td>\n","      <td>0.826864</td>\n","      <td>0.850224</td>\n","      <td>0.838382</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading builder script: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d47c68fbbaf411597b382bf597f392c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r  8%|▊         | 1/12 [12:02<2:12:26, 722.44s/trial, best loss: -0.8383816051293389]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 13:22, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.044100</td>\n","      <td>0.100207</td>\n","      <td>0.815089</td>\n","      <td>0.791330</td>\n","      <td>0.803034</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.040600</td>\n","      <td>0.108174</td>\n","      <td>0.825643</td>\n","      <td>0.821076</td>\n","      <td>0.823353</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.020000</td>\n","      <td>0.114458</td>\n","      <td>0.828738</td>\n","      <td>0.849178</td>\n","      <td>0.838834</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 17%|█▋        | 2/12 [25:39<2:09:39, 777.92s/trial, best loss: -0.8388335179032853]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 13:21, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.051900</td>\n","      <td>0.098099</td>\n","      <td>0.818537</td>\n","      <td>0.813154</td>\n","      <td>0.815837</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.034200</td>\n","      <td>0.108257</td>\n","      <td>0.830836</td>\n","      <td>0.828849</td>\n","      <td>0.829841</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.017300</td>\n","      <td>0.119080</td>\n","      <td>0.826112</td>\n","      <td>0.843647</td>\n","      <td>0.834788</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 25%|██▌       | 3/12 [39:14<1:59:16, 795.13s/trial, best loss: -0.8388335179032853]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5754' max='5754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5754/5754 11:40, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.039100</td>\n","      <td>0.094385</td>\n","      <td>0.806828</td>\n","      <td>0.837220</td>\n","      <td>0.821743</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.027800</td>\n","      <td>0.108474</td>\n","      <td>0.817944</td>\n","      <td>0.840807</td>\n","      <td>0.829218</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.013800</td>\n","      <td>0.119901</td>\n","      <td>0.822840</td>\n","      <td>0.840060</td>\n","      <td>0.831361</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 33%|███▎      | 4/12 [51:10<1:41:49, 763.64s/trial, best loss: -0.8388335179032853]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5754' max='5754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5754/5754 11:37, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.040400</td>\n","      <td>0.096233</td>\n","      <td>0.816810</td>\n","      <td>0.825112</td>\n","      <td>0.820940</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.026800</td>\n","      <td>0.103557</td>\n","      <td>0.811997</td>\n","      <td>0.843797</td>\n","      <td>0.827591</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.013100</td>\n","      <td>0.119147</td>\n","      <td>0.829025</td>\n","      <td>0.843647</td>\n","      <td>0.836272</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 42%|████▏     | 5/12 [1:03:03<1:26:57, 745.35s/trial, best loss: -0.8388335179032853]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.033900</td>\n","      <td>0.096309</td>\n","      <td>0.828423</td>\n","      <td>0.832138</td>\n","      <td>0.830276</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.022800</td>\n","      <td>0.103959</td>\n","      <td>0.824971</td>\n","      <td>0.850374</td>\n","      <td>0.837480</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.012100</td>\n","      <td>0.115516</td>\n","      <td>0.831309</td>\n","      <td>0.849327</td>\n","      <td>0.840222</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 50%|█████     | 6/12 [1:15:05<1:13:44, 737.47s/trial, best loss: -0.8402218114602588]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:44, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.033900</td>\n","      <td>0.098631</td>\n","      <td>0.828236</td>\n","      <td>0.833931</td>\n","      <td>0.831074</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.023000</td>\n","      <td>0.103365</td>\n","      <td>0.824470</td>\n","      <td>0.855157</td>\n","      <td>0.839533</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.012700</td>\n","      <td>0.114783</td>\n","      <td>0.829045</td>\n","      <td>0.852466</td>\n","      <td>0.840593</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 58%|█████▊    | 7/12 [1:27:07<1:01:02, 732.48s/trial, best loss: -0.840592527083794] "]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 13:17, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.049800</td>\n","      <td>0.096462</td>\n","      <td>0.809453</td>\n","      <td>0.814051</td>\n","      <td>0.811745</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.035800</td>\n","      <td>0.112934</td>\n","      <td>0.819001</td>\n","      <td>0.828550</td>\n","      <td>0.823748</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.017600</td>\n","      <td>0.119417</td>\n","      <td>0.825066</td>\n","      <td>0.835426</td>\n","      <td>0.830214</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 67%|██████▋   | 8/12 [1:40:39<50:31, 757.79s/trial, best loss: -0.840592527083794]  "]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:43, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.035000</td>\n","      <td>0.096184</td>\n","      <td>0.827612</td>\n","      <td>0.836024</td>\n","      <td>0.831797</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.020900</td>\n","      <td>0.103945</td>\n","      <td>0.822383</td>\n","      <td>0.851271</td>\n","      <td>0.836577</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.011400</td>\n","      <td>0.116258</td>\n","      <td>0.829155</td>\n","      <td>0.850224</td>\n","      <td>0.839557</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 75%|███████▌  | 9/12 [1:52:39<37:18, 746.13s/trial, best loss: -0.840592527083794]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 13:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.043700</td>\n","      <td>0.105039</td>\n","      <td>0.806384</td>\n","      <td>0.804335</td>\n","      <td>0.805358</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.040100</td>\n","      <td>0.111479</td>\n","      <td>0.824840</td>\n","      <td>0.808072</td>\n","      <td>0.816370</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.020000</td>\n","      <td>0.124998</td>\n","      <td>0.829265</td>\n","      <td>0.837818</td>\n","      <td>0.833519</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 83%|████████▎ | 10/12 [2:06:14<25:34, 767.18s/trial, best loss: -0.840592527083794]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 13:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.051600</td>\n","      <td>0.109468</td>\n","      <td>0.822748</td>\n","      <td>0.795815</td>\n","      <td>0.809057</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.033900</td>\n","      <td>0.109853</td>\n","      <td>0.827704</td>\n","      <td>0.835127</td>\n","      <td>0.831399</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.017200</td>\n","      <td>0.122048</td>\n","      <td>0.830787</td>\n","      <td>0.837369</td>\n","      <td>0.834065</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 92%|█████████▏| 11/12 [2:19:49<13:01, 781.75s/trial, best loss: -0.840592527083794]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2122140183.py:65: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.032200</td>\n","      <td>0.105476</td>\n","      <td>0.821339</td>\n","      <td>0.830792</td>\n","      <td>0.826038</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.025100</td>\n","      <td>0.101783</td>\n","      <td>0.820688</td>\n","      <td>0.845590</td>\n","      <td>0.832953</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.012800</td>\n","      <td>0.117092</td>\n","      <td>0.829479</td>\n","      <td>0.847085</td>\n","      <td>0.838190</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:12]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["100%|██████████| 12/12 [2:31:54<00:00, 759.52s/trial, best loss: -0.840592527083794]\n","\n","============================================================\n","BEST HYPERPARAMETERS FOUND\n","============================================================\n","Best eval micro F1 : 0.8406\n","Batch size         : 32\n","Warmup ratio       : 0.1\n","============================================================\n"]}]},{"cell_type":"code","source":["# Create a model with the best hyper parameters found.\n","# ------------------------------------------------------------\n","# Hyper-parameters\n","# ------------------------------------------------------------\n","LEARNING_RATE = 6.040686648207059e-05 #From paper\n","EPOCHS        = 3                     #From paper\n","WEIGHT_DECAY  = 0.01                   #From paper\n","BATCH_SIZE    = 32                     #Best parameter from above\n","WARMUP_RATIO  = 0.1                    #Best parameter from above\n","SEED          = 123\n","OUTPUT_DIR    = \"./tuned_ner_model\"\n","\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=LEARNING_RATE,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    num_train_epochs=EPOCHS,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_ratio=WARMUP_RATIO,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    seed=SEED,\n","    logging_steps=10,\n","    save_total_limit=2,\n","    report_to=[],\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenised[\"train\"],\n","    eval_dataset=tokenised[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()\n","\n","#Save the model\n","trainer.save_model(OUTPUT_DIR)\n","tokenizer.save_pretrained(OUTPUT_DIR)\n","print(f\"\\nModel saved to {OUTPUT_DIR}\")\n","\n"],"metadata":{"id":"TR6zPHedsWh0","colab":{"base_uri":"https://localhost:8080/","height":307},"executionInfo":{"status":"ok","timestamp":1763710717853,"user_tz":480,"elapsed":709369,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"748dc9c8-c826-4307-ff48-99204cf5c844"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-24901224.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"stream","name":"stdout","text":["\n","STARTING TRAINING ...\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:45, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.013500</td>\n","      <td>0.114615</td>\n","      <td>0.845205</td>\n","      <td>0.808819</td>\n","      <td>0.826612</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.020000</td>\n","      <td>0.112511</td>\n","      <td>0.822129</td>\n","      <td>0.846338</td>\n","      <td>0.834058</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.012000</td>\n","      <td>0.128031</td>\n","      <td>0.831515</td>\n","      <td>0.854260</td>\n","      <td>0.842734</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Model saved to ./tuned_ner_model\n"]}]},{"cell_type":"code","source":["#Quick test\n","from transformers import pipeline\n","import unicodedata\n","\n","ner = pipeline(\"ner\", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR,\n","               aggregation_strategy=\"simple\")\n","\n","txt = unicodedata.normalize(\"NFC\", \"\"\"\n","  ᾿Ανέστη δὲ βασιλεὺς ἕτερος ἐπ᾿ Αἴγυπτον, ὃς οὐκ ᾔδει τὸν ᾿Ιωσήφ.\n","  εἶπε δὲ τῷ ἔθνει αὐτοῦ· ἰδοὺ τὸ γένος τῶν υἱῶν ᾿Ισραὴλ μέγα πλῆθος καὶ ἰσχύει ὑπὲρ ἡμᾶς·\n","  δεῦτε οὖν κατασοφισώμεθα αὐτούς, μή ποτε πληθυνθῇ, καὶ ἡνίκα ἂν συμβῇ ἡμῖν πόλεμος,\n","  προστεθήσονται καὶ οὗτοι πρὸς τοὺς ὑπεναντίους καὶ ἐκπολεμήσαντες ἡμᾶς ἐξελεύσονται ἐκ τῆς γῆς.\n","  καὶ ἐπέστησεν αὐτοῖς ἐπιστάτας τῶν ἔργων, ἵνα κακώσωσιν αὐτοὺς ἐν τοῖς ἔργοις· καὶ Ισραήλᾠκοδόμησαν πόλεις ὀχυρὰς τῷ Φαραώ, τήν τε Πειθὼ καὶ Ῥαμεσσῆ καὶ ῎Ων, ἥ ἐστιν ῾Ηλιούπολις.\n","  καθότι δὲ αὐτοὺς ἐταπείνουν, τοσούτῳ πλείους ἐγίγνοντο, καὶ ἴσχυον σφόδρα σφόδρα· καὶ ἐβδελύσσοντο οἱ Αἰγύπτιοι ἀπὸ τῶν υἱῶν ᾿.\n","  καὶ κατεδυνάστευον οἱ Αἰγύπτιοι τοὺς υἱοὺς ᾿Ισραὴλ βίᾳ καὶ κατωδύνων αὐτῶν τὴν ζωὴν ἐν τοῖς ἔργοις τοῖς σκληροῖς, τῷ πηλῷ καὶ τῇ πλινθείᾳ καὶ πᾶσι τοῖς ἔργοις τοῖς ἐν τοῖς πεδίοις, κατὰ πάντα τὰ ἔργα, ὧν κατεδουλοῦντο αὐτοὺς μετὰ βίας.\n","\"\"\")\n","\n","merged_results = []\n","\n","for r in ner(txt):\n","    if r['word'].startswith(\"##\"):\n","        merged_results[-1]['word'] += r['word'][2:]  # remove ## and join the subwords together instead of splitting it\n","        merged_results[-1]['score'] = max(merged_results[-1]['score'], r['score'])\n","    else:\n","        merged_results.append(r)\n","\n","for r in merged_results:\n","    print(f\"{r['word']:<20} → {r['entity_group']:<6} ({r['score']:.3f})\")\n"],"metadata":{"id":"kavnuHZksWk0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763710719927,"user_tz":480,"elapsed":2021,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"3d9d38de-d86a-4ba1-bd75-736a52bcc040"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["αιγυπτον             → LOC    (0.987)\n","φαραωω               → PERS   (0.999)\n","ραμεσση              → LOC    (0.908)\n","αιγυπτιοι            → GRP    (1.000)\n","αιγυπτιοι            → GRP    (1.000)\n"]}]},{"cell_type":"code","source":["#Final test F1 score\n","FINAL_MODEL_DIR = OUTPUT_DIR\n","tokenizer_test = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR)\n","model_test = AutoModelForTokenClassification.from_pretrained(FINAL_MODEL_DIR)\n","\n","trainer_test = Trainer(\n","    model=model_test,\n","    args=TrainingArguments(\n","        output_dir=\"./temp_eval\",\n","        per_device_eval_batch_size=32,\n","    ),\n","    eval_dataset=tokenised['test'], #used the Test dataset that was previously processed in same manner as the Train and Val\n","    tokenizer=tokenizer_test,\n","    data_collator=DataCollatorForTokenClassification(tokenizer_test),\n","    compute_metrics=compute_metrics,\n",")\n","\n","print(\"Running official test set evaluation...\")\n","results = trainer.evaluate()\n","\n","print(\"\\n\" + \"═\" * 60)\n","print(\"FINAL OFFICIAL TEST RESULTS (same as paper)\")\n","print(\"═\" * 60)\n","print(f\"Precision : {results['eval_precision']:.4f}\")\n","print(f\"Recall    : {results['eval_recall']:.4f}\")\n","print(f\"Micro F1  : {results['eval_f1']:.4f}\")\n","print(\"═\" * 60)\n","\n","if results['eval_f1'] > 0.826:\n","    print(\"We did better than the paper's 0.826!\")\n","else:\n","    print(\"Close to or matches the original paper result.\")"],"metadata":{"id":"FSoI9MzjsWnp","colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"status":"ok","timestamp":1763710751108,"user_tz":480,"elapsed":31176,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"37251fb8-8dde-49f5-c6a0-5a5a203a1e84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3241392163.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer_test = Trainer(\n"]},{"output_type":"stream","name":"stdout","text":["Running official test set evaluation...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","════════════════════════════════════════════════════════════\n","FINAL OFFICIAL TEST RESULTS (same as paper)\n","════════════════════════════════════════════════════════════\n","Precision : 0.8315\n","Recall    : 0.8543\n","Micro F1  : 0.8427\n","════════════════════════════════════════════════════════════\n","We did better than the paper's 0.826!\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"DqHRMscmv5cR"},"execution_count":null,"outputs":[]}]}
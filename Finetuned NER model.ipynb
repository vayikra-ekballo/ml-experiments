{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1igUEWoZG9974U9P25Si6JXxtBH-DxNBg","timestamp":1763190472884}],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b47a327f584945f795d5fc572438b21d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f675bbb5e2c34913baedc3b59e11ee57","IPY_MODEL_b87ba2b44026459bb2c07be8281f48b6","IPY_MODEL_cce4cf7eb16642888ddeab720c5f3f54"],"layout":"IPY_MODEL_42b9750a5e4547d79d5863227bf06be1"}},"f675bbb5e2c34913baedc3b59e11ee57":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df1d074e87254ed0a7ac3da30d75350d","placeholder":"​","style":"IPY_MODEL_56763f2a9e2a416a9ca45a7e83153e21","value":"Map: 100%"}},"b87ba2b44026459bb2c07be8281f48b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d503863475ea42d198782af6e143704e","max":30686,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b3514ba148f546c48fd709f61dd2020e","value":30686}},"cce4cf7eb16642888ddeab720c5f3f54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c56c6bcf342741fb9019e90e10973ad5","placeholder":"​","style":"IPY_MODEL_451782ab9fd748aaa0738fc36eed48fa","value":" 30686/30686 [00:03&lt;00:00, 7993.74 examples/s]"}},"42b9750a5e4547d79d5863227bf06be1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df1d074e87254ed0a7ac3da30d75350d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56763f2a9e2a416a9ca45a7e83153e21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d503863475ea42d198782af6e143704e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3514ba148f546c48fd709f61dd2020e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c56c6bcf342741fb9019e90e10973ad5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"451782ab9fd748aaa0738fc36eed48fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5b1d3b3ac564e50a694bd4d25d8c5d2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fbfacd056fe54bd1b95ea4ca69a47251","IPY_MODEL_51897958527b46cc920468e996e4c20d","IPY_MODEL_50faf8e3cc80477690a27ecfeb468190"],"layout":"IPY_MODEL_b416920bbce747a3ae0057f8993359f3"}},"fbfacd056fe54bd1b95ea4ca69a47251":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1321dcea573447a79f0e8911084cd2d2","placeholder":"​","style":"IPY_MODEL_d9f60b7b94f94bf3acbd1d727be8c981","value":"Map: 100%"}},"51897958527b46cc920468e996e4c20d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_801e28da4dc841979d8f272ef333b9e5","max":4434,"min":0,"orientation":"horizontal","style":"IPY_MODEL_951283295ae048649eee3ba3e097f432","value":4434}},"50faf8e3cc80477690a27ecfeb468190":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_baff071738de48ba8dec3ec2314b8863","placeholder":"​","style":"IPY_MODEL_6af1b25ba5e24cb79eee022a481bb356","value":" 4434/4434 [00:00&lt;00:00, 8474.14 examples/s]"}},"b416920bbce747a3ae0057f8993359f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1321dcea573447a79f0e8911084cd2d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9f60b7b94f94bf3acbd1d727be8c981":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"801e28da4dc841979d8f272ef333b9e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"951283295ae048649eee3ba3e097f432":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"baff071738de48ba8dec3ec2314b8863":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6af1b25ba5e24cb79eee022a481bb356":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62daaf0f26ec4ebea32bee2bc1b67971":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df89b07271c44225824b931f3db4b494","IPY_MODEL_a764104fd48b4e58948347f64e001a54","IPY_MODEL_17e56ccbb63c436eb98cd1e79a57725a"],"layout":"IPY_MODEL_f912771351d241119fbc44a93f47e18d"}},"df89b07271c44225824b931f3db4b494":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cccc0d7de0514d13b30b1c094b1b4653","placeholder":"​","style":"IPY_MODEL_4e1815eec150406c9ad56ed428da44bf","value":"Map: 100%"}},"a764104fd48b4e58948347f64e001a54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb492f65583a4cfa8a74e3d7af794dda","max":4701,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7097d2b846d8422d895612b78f64c8bb","value":4701}},"17e56ccbb63c436eb98cd1e79a57725a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59acc8d2099b4f85933d76df7fc58cad","placeholder":"​","style":"IPY_MODEL_5e76d656284d4c3fbd04411477f48c63","value":" 4701/4701 [00:00&lt;00:00, 8765.37 examples/s]"}},"f912771351d241119fbc44a93f47e18d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cccc0d7de0514d13b30b1c094b1b4653":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e1815eec150406c9ad56ed428da44bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb492f65583a4cfa8a74e3d7af794dda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7097d2b846d8422d895612b78f64c8bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59acc8d2099b4f85933d76df7fc58cad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e76d656284d4c3fbd04411477f48c63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["To do:\n","\n","\n","1.   Test more hyperparameters to get metrics in 0.9 - HuggingFace Trainer has native support for hyperparameter search using either Optuna, Ray Tune, or Weights & Biases.\n","2.   Data augmentation - use another LLM to do NER categorization of text. and then add that to the training model.\n","\n"],"metadata":{"id":"jRXTyqRIXL04"}},{"cell_type":"markdown","source":["Another relevant application of large language models (LLMs) in linguistic tasks is Named Entity Recognition (NER). Recent work by Beersmans et al. (2024) demonstrates this by combining transformer-based models with domain-specific knowledge to identify individuals in Ancient Greek texts. Their study, “Gotta catch ’em all!: Retrieving people in Ancient Greek texts combining transformer models and domain knowledge,” was presented at the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024) and provides a strong example of how modern NLP techniques can be adapted for historical languages.\n","\n","The authors built a model for Ancient greek NER task with F1 score of 0.826, which is State-of-Art as of current. Since Ancient Greek is a low-resource, highly inflected ancient language with limited annotated corpora (e.g., the dataset used here aggregates ~5,579 test tokens from projects like First1KGreek and NERAncientGreekML4AL). Unlike modern high-resource languages (e.g., English, where CoNLL-2003 NER F1 scores exceed 0.93), ancient languages suffer from data scarcity, orthographic variations (e.g., diacritics, dialects), and domain noise (e.g., fragmentary inscriptions or papyri). SOTA in this niche is typically in the 0.80–0.89 range for transformer-based models on similar tasks.\n","\n","We attempted to do hyperparameters tuning for a better performance - there are 2 hyperparameters not tested in the original paper - Warmup ratio and batch size. Both of these are sensitive to transformer tuning.\n","Large batch size reduces noise, resulting in better token representation. This is particulary useful for complex morphologically rich languages like Ancient Greek. However, smaller batches tend to act like regularization allowing for beter generalization. Thus tuning of the batch size is to find the balance between overfitting and token representation.\n","\n","[describe warm up]\n","\n","Data augmentation is another potential approach to improving the model’s F1 score, but it is not practical for this project. Given our limited resources, we would need multiple models to first annotate the English translations of the Ancient Greek sentences and then align those annotations back to the corresponding Koine Greek tokens to infer NER labels. Even after this automated pipeline, human verification would still be required to ensure label accuracy. Producing a dataset of roughly 100,000 Koine Greek tokens under these constraints would be extremely time-consuming and effectively not feasible within the scope of this project."],"metadata":{"id":"dIhPGk1cYD6t"}},{"cell_type":"code","source":["#!pip install --upgrade transformers\n","!pip install -q transformers datasets seqeval torch tqdm evaluate\n"],"metadata":{"id":"7XUwO3KBXLAN","executionInfo":{"status":"ok","timestamp":1763432330836,"user_tz":480,"elapsed":4352,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nVU1STPnCvtA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7A0B2lSC1T2","executionInfo":{"status":"ok","timestamp":1763437160567,"user_tz":480,"elapsed":17959,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"eaed8ef8-24eb-4199-c778-60f4e27b1e72"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SlwkpUQKjK60","executionInfo":{"status":"ok","timestamp":1763432344167,"user_tz":480,"elapsed":12561,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"9fd32ac7-d727-4491-d05b-94582f72eef6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'NERAncientGreekML4AL'...\n","remote: Enumerating objects: 251, done.\u001b[K\n","remote: Total 251 (delta 0), reused 0 (delta 0), pack-reused 251 (from 3)\u001b[K\n","Receiving objects: 100% (251/251), 106.47 MiB | 16.09 MiB/s, done.\n","Resolving deltas: 100% (98/98), done.\n","Updating files: 100% (199/199), done.\n","Downloading Data/homogenisation/full_dataset_FINAL.csv (113 MB)\n","Error downloading object: Data/homogenisation/full_dataset_FINAL.csv (82d984c): Smudge error: Error downloading Data/homogenisation/full_dataset_FINAL.csv (82d984c506fbdcea63db80edc6d34c42f5128b2de3a34df706c6ecae87f02254): batch response: This repository exceeded its LFS budget. The account responsible for the budget should increase it to restore access.\n","\n","Errors logged to /content/NERAncientGreekML4AL/.git/lfs/logs/20251118T021904.003062136.log\n","Use `git lfs logs last` to view the log.\n","error: external filter 'git-lfs filter-process' failed\n","fatal: Data/homogenisation/full_dataset_FINAL.csv: smudge filter lfs failed\n","warning: Clone succeeded, but checkout failed.\n","You can inspect what was checked out with 'git status'\n","and retry with 'git restore --source=HEAD :/'\n","\n","/content/NERAncientGreekML4AL\n","final_dataset/normal/test.conll   final_dataset/normal/val.conll\n","final_dataset/normal/train.conll\n"]}],"source":["\n","\n","# Clone the repo\n","#!git clone https://github.com/NER-AncientLanguages/NERAncientGreekML4AL.git\n","#%cd NERAncientGreekML4AL\n","\n","# Verify data exists\n","#!ls final_dataset/normal/*.conll"]},{"cell_type":"markdown","source":[],"metadata":{"id":"FSol7vZB9QyT"}},{"cell_type":"code","source":["import os, warnings, unicodedata, numpy as np\n","from pathlib import Path\n","from datasets import Dataset, DatasetDict\n","from transformers import (\n","    AutoTokenizer, AutoModelForTokenClassification,\n","    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",")\n","\n","import evaluate\n","from seqeval.metrics import classification_report\n","\n","def read_conll(p: Path):\n","    \"\"\"\n","    Parse CoNLL with format:\n","        [line_id]  token  [POS]  NER\n","    Example:\n","        110089790\tβίβλος\tO\n","    Returns: {\"tokens\": [...], \"ner_tags\": [...]}\n","    \"\"\"\n","    sents, labs = [], []\n","    with p.open(encoding=\"utf-8\") as f:\n","        sent, lab = [], []\n","        for i, raw in enumerate(f, 1):\n","            line = raw.strip()\n","            if not line or line.startswith(\"#\"):\n","                if sent:\n","                    sents.append(sent)\n","                    labs.append(lab)\n","                    sent, lab = [], []\n","                continue\n","\n","            # Split on whitespace (handles tabs and spaces)\n","            parts = line.split()\n","            if len(parts) < 2:\n","                print(f\"Warning: Line {i} in {p.name} has <2 columns → SKIPPED\")\n","                print(f\"    → {line!r}\")\n","                continue\n","\n","            if len(parts) == 2:\n","                token = parts[0]\n","                ner   = parts[1]\n","            else:\n","                token = parts[1]   # skip ID\n","                ner   = parts[-1]  # last column is NER\n","\n","            sent.append(unicodedata.normalize(\"NFC\", token))\n","            lab.append(ner)\n","\n","        if sent:\n","            sents.append(sent)\n","            labs.append(lab)\n","\n","    print(f\"Loaded {len(sents)} sentences from {p.name}\")\n","    return {\"tokens\": sents, \"ner_tags\": labs}\n","\n","# load data\n","train_path = Path(\"/content/drive/My Drive/Deep Learning Group Project/train.conll\")\n","val_path   = Path(\"/content/drive/My Drive/Deep Learning Group Project/val.conll\")\n","test_path  = Path(\"/content/drive/My Drive/Deep Learning Group Project/test.conll\")\n","\n","raw = {\n","    \"train\": read_conll(train_path),\n","    \"validation\": read_conll(val_path),\n","    \"test\": read_conll(test_path),\n","}\n","data = DatasetDict({k: Dataset.from_dict(v) for k, v in raw.items()})\n","\n","#Model name -------------------------------------------------------------\n","model_name = \"Marijke/AG_BERT_hypopt_NER\"\n","tokenizer  = AutoTokenizer.from_pretrained(model_name)\n","#------------------------------------------------------------------------\n","\n","all_labels = sorted({l for s in data[\"train\"][\"ner_tags\"] for l in s})\n","label2id   = {l: i for i, l in enumerate(all_labels)}\n","id2label   = {i: l for l, i in label2id.items()}\n","\n","#tokenise + align labels\n","def tokenise_align(example):\n","    tok = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n","    aligned = []\n","    for i, labs in enumerate(example[\"ner_tags\"]):\n","        word_ids = tok.word_ids(batch_index=i)\n","        prev = None\n","        ids  = []\n","        for wid in word_ids:\n","            if wid is None:\n","                ids.append(-100)\n","            elif wid != prev:\n","                ids.append(label2id[labs[wid]])\n","            else:\n","                ids.append(-100)               # sub-word → ignore\n","            prev = wid\n","        aligned.append(ids)\n","    tok[\"labels\"] = aligned\n","    return tok\n","\n","tokenised = data.map(tokenise_align, batched=True,\n","                     remove_columns=data[\"train\"].column_names)\n","\n","\n","model = AutoModelForTokenClassification.from_pretrained(\n","    model_name,\n","    num_labels=len(all_labels),\n","    id2label=id2label,\n","    label2id=label2id,\n",")\n","\n","collator = DataCollatorForTokenClassification(tokenizer)\n","\n","\n","def compute_metrics(p):\n","    preds, labels = p\n","    preds = np.argmax(preds, axis=2)\n","\n","    true_labels = []\n","    pred_labels = []\n","\n","    for prediction, label in zip(preds, labels):\n","        true_seq = [id2label[l] for l in label if l != -100]\n","        pred_seq = [id2label[pred] for pred, l in zip(prediction, label) if l != -100]\n","        if true_seq:  # Only add if not empty\n","            true_labels.append(true_seq)\n","            pred_labels.append(pred_seq)\n","\n","    if not true_labels:\n","        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n","\n","\n","    metric = evaluate.load(\"seqeval\")\n","    results = metric.compute(predictions=pred_labels, references=true_labels)\n","\n","    return {\n","      \"precision\": results[\"overall_precision\"],\n","      \"recall\": results['overall_recall'],\n","      \"f1\": results[\"overall_f1\"]\n","    }\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":167,"referenced_widgets":["b47a327f584945f795d5fc572438b21d","f675bbb5e2c34913baedc3b59e11ee57","b87ba2b44026459bb2c07be8281f48b6","cce4cf7eb16642888ddeab720c5f3f54","42b9750a5e4547d79d5863227bf06be1","df1d074e87254ed0a7ac3da30d75350d","56763f2a9e2a416a9ca45a7e83153e21","d503863475ea42d198782af6e143704e","b3514ba148f546c48fd709f61dd2020e","c56c6bcf342741fb9019e90e10973ad5","451782ab9fd748aaa0738fc36eed48fa","c5b1d3b3ac564e50a694bd4d25d8c5d2","fbfacd056fe54bd1b95ea4ca69a47251","51897958527b46cc920468e996e4c20d","50faf8e3cc80477690a27ecfeb468190","b416920bbce747a3ae0057f8993359f3","1321dcea573447a79f0e8911084cd2d2","d9f60b7b94f94bf3acbd1d727be8c981","801e28da4dc841979d8f272ef333b9e5","951283295ae048649eee3ba3e097f432","baff071738de48ba8dec3ec2314b8863","6af1b25ba5e24cb79eee022a481bb356","62daaf0f26ec4ebea32bee2bc1b67971","df89b07271c44225824b931f3db4b494","a764104fd48b4e58948347f64e001a54","17e56ccbb63c436eb98cd1e79a57725a","f912771351d241119fbc44a93f47e18d","cccc0d7de0514d13b30b1c094b1b4653","4e1815eec150406c9ad56ed428da44bf","fb492f65583a4cfa8a74e3d7af794dda","7097d2b846d8422d895612b78f64c8bb","59acc8d2099b4f85933d76df7fc58cad","5e76d656284d4c3fbd04411477f48c63"]},"id":"M48lINeotYTu","executionInfo":{"status":"ok","timestamp":1763437378788,"user_tz":480,"elapsed":9607,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"bebfa714-c58c-4ef8-c1c7-6f8ec93e88d5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded 30686 sentences from train.conll\n","Loaded 4434 sentences from val.conll\n","Loaded 4701 sentences from test.conll\n"]},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/30686 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b47a327f584945f795d5fc572438b21d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4434 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5b1d3b3ac564e50a694bd4d25d8c5d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/4701 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62daaf0f26ec4ebea32bee2bc1b67971"}},"metadata":{}}]},{"cell_type":"markdown","source":["Hyperparameter tuning was performed using Hyperopt. Although Hyperopt is less commonly used today, we chose it to maintain consistency with the methodology described in the referenced paper."],"metadata":{"id":"ia46xfbcZysY"}},{"cell_type":"code","source":["import os\n","import numpy as np\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForTokenClassification,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForTokenClassification,\n",")\n","import evaluate\n","from hyperopt import hp, fmin, tpe, Trials, STATUS_OK, STATUS_FAIL\n","from hyperopt.early_stop import no_progress_loss\n","\n","# -------------------------------\n","# HYPEROPT SEARCH SPACE\n","# Include 2 new parameters that were not tried in the paper - batch size and warmup ratio\n","# -------------------------------\n","FIXED_LR = 6.040686648207059e-05\n","FIXED_WD = 0.01\n","FIXED_EPOCH = 3\n","\n","space = {\n","    \"batch_size\":    hp.choice(\"batch_size\", [8, 16, 32]),            # 3 options\n","    \"warmup_ratio\":  hp.choice(\"warmup_ratio\", [0.0, 0.06, 0.1, 0.2]), # 4 options\n","    \"seed\": 123 #for reproducibility\n","}\n","\n","# -------------------------------\n","# OBJECTIVE FUNCTION used by Hyperopt to test parameters\n","# -------------------------------\n","def objective(params):\n","  # we are keeping these 3 hyperparameters from the paper itself as they have found the optimal values for the Learning Rate, Weight Decay\n","  # and number of training epoch\n","\n","    try:\n","\n","        # Corrected: Directly use the values from params, as hp.choice returns the value itself, not an index\n","        batch_size = params[\"batch_size\"]\n","        warmup_ratio = params[\"warmup_ratio\"]\n","\n","        model_for_trial = AutoModelForTokenClassification.from_pretrained(\n","            model_name,\n","            num_labels=len(all_labels),\n","            id2label=id2label,\n","            label2id=label2id,\n","        )\n","\n","\n","        total_steps = int(len(tokenised[\"train\"]) / batch_size * FIXED_EPOCH)\n","        warmup_steps = int(total_steps * warmup_ratio)\n","\n","        training_args = TrainingArguments(\n","            output_dir=f\"./hyperopt_trial_{int(FIXED_EPOCH)}_{batch_size}_{FIXED_LR:.2e}\",\n","            num_train_epochs=FIXED_EPOCH,\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size * 2,\n","            learning_rate=FIXED_LR,\n","            weight_decay=FIXED_WD,\n","            warmup_steps=warmup_steps,\n","            lr_scheduler_type=\"linear\",\n","            eval_strategy=\"epoch\",\n","            save_strategy=\"epoch\",\n","            logging_strategy=\"epoch\",\n","            load_best_model_at_end=True,\n","            metric_for_best_model=\"f1\",\n","            greater_is_better=True,\n","            report_to=\"none\",\n","            seed=params[\"seed\"],\n","            dataloader_num_workers=4,\n","            disable_tqdm=False,\n","        )\n","\n","        trainer = Trainer(\n","            model=model_for_trial,\n","            args=training_args,\n","            train_dataset=tokenised[\"train\"],\n","            eval_dataset=tokenised[\"validation\"],\n","            tokenizer=tokenizer,\n","            data_collator=collator,\n","            compute_metrics=compute_metrics,\n","        )\n","\n","        trainer.train()\n","        metrics = trainer.evaluate()\n","\n","        return {\n","            \"loss\": -metrics[\"eval_f1\"],\n","            \"status\": STATUS_OK,\n","            \"eval_f1\": metrics[\"eval_f1\"],\n","            \"params\": params,\n","        }\n","\n","    except Exception as e:\n","        print(f\"Trial failed: {e}\")\n","        return {\"loss\": 10.0, \"status\": STATUS_FAIL}\n","\n","# -------------------------------\n","# RUN HYPEROPT\n","# -------------------------------\n","trials = Trials()\n","\n","best = fmin(\n","    fn=objective,\n","    space=space,\n","    algo=tpe.suggest,\n","    max_evals=12,\n","    trials=trials,\n","    rstate=np.random.default_rng(42),\n","    show_progressbar=True,\n",")\n","\n","# -------------------------------\n","# PRINT BEST RESULT\n","# -------------------------------\n","best_trial = trials.best_trial\n","print(\"\\n\" + \"=\"*60)\n","print(\"BEST HYPERPARAMETERS FOUND\")\n","print(\"=\"*60)\n","print(f\"Best eval micro F1 : {best_trial['result']['eval_f1']:.4f}\")\n","print(f\"Batch size         : {int(best_trial['result']['params']['batch_size'])}\")\n","print(f\"Warmup ratio       : {best_trial['result']['params']['warmup_ratio']}\")\n","print(\"=\"*60)\n","\n","# Optional: retrain on full train+val with best params and evaluate on test set"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"I95sFuX0eG5V","executionInfo":{"status":"ok","timestamp":1763446790523,"user_tz":480,"elapsed":8834078,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"82715ea7-58d4-4b96-8a6b-c5043f1a6c50"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["\r  0%|          | 0/12 [00:00<?, ?trial/s, best loss=?]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:25, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.035000</td>\n","      <td>0.095455</td>\n","      <td>0.826412</td>\n","      <td>0.841854</td>\n","      <td>0.834061</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.020800</td>\n","      <td>0.104416</td>\n","      <td>0.825974</td>\n","      <td>0.852765</td>\n","      <td>0.839156</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.011400</td>\n","      <td>0.118280</td>\n","      <td>0.828293</td>\n","      <td>0.851570</td>\n","      <td>0.839770</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r  8%|▊         | 1/12 [11:41<2:08:33, 701.21s/trial, best loss: -0.8397700471698114]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 12:56, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.044100</td>\n","      <td>0.115932</td>\n","      <td>0.812135</td>\n","      <td>0.768311</td>\n","      <td>0.789615</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.040600</td>\n","      <td>0.108344</td>\n","      <td>0.824427</td>\n","      <td>0.823318</td>\n","      <td>0.823873</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.020200</td>\n","      <td>0.115440</td>\n","      <td>0.830653</td>\n","      <td>0.841704</td>\n","      <td>0.836142</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 17%|█▋        | 2/12 [24:50<2:05:28, 752.89s/trial, best loss: -0.8397700471698114]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 12:52, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.051900</td>\n","      <td>0.101465</td>\n","      <td>0.818250</td>\n","      <td>0.809567</td>\n","      <td>0.813885</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.033500</td>\n","      <td>0.111172</td>\n","      <td>0.829559</td>\n","      <td>0.821375</td>\n","      <td>0.825447</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.016900</td>\n","      <td>0.122214</td>\n","      <td>0.831281</td>\n","      <td>0.837369</td>\n","      <td>0.834314</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 25%|██▌       | 3/12 [37:55<1:55:09, 767.71s/trial, best loss: -0.8397700471698114]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5754' max='5754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5754/5754 11:17, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.039300</td>\n","      <td>0.093850</td>\n","      <td>0.814826</td>\n","      <td>0.841256</td>\n","      <td>0.827830</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.027800</td>\n","      <td>0.109792</td>\n","      <td>0.815710</td>\n","      <td>0.842900</td>\n","      <td>0.829082</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.014000</td>\n","      <td>0.120506</td>\n","      <td>0.827865</td>\n","      <td>0.841106</td>\n","      <td>0.834433</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 33%|███▎      | 4/12 [49:26<1:38:18, 737.33s/trial, best loss: -0.8397700471698114]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='5754' max='5754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [5754/5754 11:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.040400</td>\n","      <td>0.097479</td>\n","      <td>0.822495</td>\n","      <td>0.811061</td>\n","      <td>0.816738</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.026800</td>\n","      <td>0.102771</td>\n","      <td>0.816739</td>\n","      <td>0.846039</td>\n","      <td>0.831131</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.013300</td>\n","      <td>0.118005</td>\n","      <td>0.831167</td>\n","      <td>0.841106</td>\n","      <td>0.836107</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 00:10]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 42%|████▏     | 5/12 [1:01:00<1:24:12, 721.85s/trial, best loss: -0.8397700471698114]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:26, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.033800</td>\n","      <td>0.097444</td>\n","      <td>0.829641</td>\n","      <td>0.828401</td>\n","      <td>0.829020</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.023100</td>\n","      <td>0.104894</td>\n","      <td>0.824975</td>\n","      <td>0.853214</td>\n","      <td>0.838857</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.011900</td>\n","      <td>0.116623</td>\n","      <td>0.830093</td>\n","      <td>0.855157</td>\n","      <td>0.842439</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 50%|█████     | 6/12 [1:12:43<1:11:31, 715.32s/trial, best loss: -0.8424385215726697]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:26, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.034000</td>\n","      <td>0.099151</td>\n","      <td>0.830648</td>\n","      <td>0.832138</td>\n","      <td>0.831392</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.022900</td>\n","      <td>0.103374</td>\n","      <td>0.820671</td>\n","      <td>0.855755</td>\n","      <td>0.837846</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.012500</td>\n","      <td>0.116794</td>\n","      <td>0.830523</td>\n","      <td>0.854111</td>\n","      <td>0.842152</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 58%|█████▊    | 7/12 [1:24:25<59:15, 711.08s/trial, best loss: -0.8424385215726697]  "]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 12:56, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.049300</td>\n","      <td>0.100252</td>\n","      <td>0.813259</td>\n","      <td>0.781166</td>\n","      <td>0.796889</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.035500</td>\n","      <td>0.110200</td>\n","      <td>0.822118</td>\n","      <td>0.834529</td>\n","      <td>0.828277</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.017600</td>\n","      <td>0.118238</td>\n","      <td>0.825474</td>\n","      <td>0.839910</td>\n","      <td>0.832629</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 67%|██████▋   | 8/12 [1:37:34<49:03, 735.85s/trial, best loss: -0.8424385215726697]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:26, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.035100</td>\n","      <td>0.093263</td>\n","      <td>0.823667</td>\n","      <td>0.840658</td>\n","      <td>0.832076</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.021000</td>\n","      <td>0.103993</td>\n","      <td>0.826864</td>\n","      <td>0.850224</td>\n","      <td>0.838382</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.011300</td>\n","      <td>0.115343</td>\n","      <td>0.827386</td>\n","      <td>0.852616</td>\n","      <td>0.839812</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 75%|███████▌  | 9/12 [1:49:16<36:15, 725.32s/trial, best loss: -0.8424385215726697]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 12:54, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.043800</td>\n","      <td>0.109722</td>\n","      <td>0.809429</td>\n","      <td>0.787892</td>\n","      <td>0.798515</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.039900</td>\n","      <td>0.106748</td>\n","      <td>0.826685</td>\n","      <td>0.827055</td>\n","      <td>0.826870</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.020100</td>\n","      <td>0.118187</td>\n","      <td>0.826151</td>\n","      <td>0.839611</td>\n","      <td>0.832827</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 83%|████████▎ | 10/12 [2:02:23<24:48, 744.33s/trial, best loss: -0.8424385215726697]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='11508' max='11508' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11508/11508 12:54, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.051500</td>\n","      <td>0.103613</td>\n","      <td>0.818583</td>\n","      <td>0.804634</td>\n","      <td>0.811548</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.034300</td>\n","      <td>0.109292</td>\n","      <td>0.823027</td>\n","      <td>0.835575</td>\n","      <td>0.829254</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.016700</td>\n","      <td>0.124583</td>\n","      <td>0.833012</td>\n","      <td>0.840359</td>\n","      <td>0.836669</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='278' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [278/278 00:09]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\r 92%|█████████▏| 11/12 [2:15:30<12:37, 757.46s/trial, best loss: -0.8424385215726697]"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1339185124.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:27, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.032200</td>\n","      <td>0.105150</td>\n","      <td>0.820698</td>\n","      <td>0.833333</td>\n","      <td>0.826967</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.025100</td>\n","      <td>0.101871</td>\n","      <td>0.819980</td>\n","      <td>0.842900</td>\n","      <td>0.831282</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.012900</td>\n","      <td>0.114957</td>\n","      <td>0.828934</td>\n","      <td>0.849626</td>\n","      <td>0.839153</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='70' max='70' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [70/70 00:11]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["100%|██████████| 12/12 [2:27:14<00:00, 736.17s/trial, best loss: -0.8424385215726697]\n","\n","============================================================\n","BEST HYPERPARAMETERS FOUND\n","============================================================\n","Best eval micro F1 : 0.8424\n","Batch size         : 32\n","Warmup ratio       : 0.1\n","============================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"77nBByjh5sF3","executionInfo":{"status":"aborted","timestamp":1763432314541,"user_tz":480,"elapsed":2,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a model with the best hyper parameters found.\n","# ------------------------------------------------------------\n","# Hyper-parameters\n","# ------------------------------------------------------------\n","LEARNING_RATE = 6.040686648207059e-05 #From paper\n","EPOCHS        = 3                     #From paper\n","WEIGHT_DECAY  = 0.01                   #From paper\n","BATCH_SIZE    = 32                     #Best parameter from above\n","WARMUP_RATIO  = 0.1                    #Best parameter from above\n","SEED          = 123\n","OUTPUT_DIR    = \"/content/drive/My Drive/Deep Learning Group Project/tuned_ner_model\"\n","\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=LEARNING_RATE,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    num_train_epochs=EPOCHS,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_ratio=WARMUP_RATIO,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    seed=SEED,\n","    logging_steps=10,\n","    save_total_limit=2,\n","    report_to=[],\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenised[\"train\"],\n","    eval_dataset=tokenised[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","#Train the model\n","print(\"\\nSTARTING TRAINING ...\\n\")\n","trainer.train()\n","\n","#Save the model\n","trainer.save_model(OUTPUT_DIR)\n","tokenizer.save_pretrained(OUTPUT_DIR)\n","print(f\"\\nModel saved to {OUTPUT_DIR}\")\n","\n"],"metadata":{"id":"oTOvrHmvzdwG","executionInfo":{"status":"ok","timestamp":1763448640142,"user_tz":480,"elapsed":700662,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"colab":{"base_uri":"https://localhost:8080/","height":314},"outputId":"6daac369-53ce-45e1-94c4-0be25a326c4b"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2008623300.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"stream","name":"stdout","text":["\n","STARTING TRAINING ...\n","\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='2877' max='2877' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2877/2877 11:38, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.030600</td>\n","      <td>0.097973</td>\n","      <td>0.827334</td>\n","      <td>0.827952</td>\n","      <td>0.827643</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.022800</td>\n","      <td>0.104885</td>\n","      <td>0.826486</td>\n","      <td>0.847982</td>\n","      <td>0.837096</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.017000</td>\n","      <td>0.113758</td>\n","      <td>0.832677</td>\n","      <td>0.853214</td>\n","      <td>0.842820</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Model saved to /content/drive/My Drive/Deep Learning Group Project/tuned_ner_model\n"]}]},{"cell_type":"code","source":["#Quick test\n","from transformers import pipeline\n","import unicodedata\n","\n","ner = pipeline(\"ner\", model=OUTPUT_DIR, tokenizer=OUTPUT_DIR,\n","               aggregation_strategy=\"simple\")\n","\n","txt = unicodedata.normalize(\"NFC\", \"\"\"\n","  ᾿Ανέστη δὲ βασιλεὺς ἕτερος ἐπ᾿ Αἴγυπτον, ὃς οὐκ ᾔδει τὸν ᾿Ιωσήφ.\n","  εἶπε δὲ τῷ ἔθνει αὐτοῦ· ἰδοὺ τὸ γένος τῶν υἱῶν ᾿Ισραὴλ μέγα πλῆθος καὶ ἰσχύει ὑπὲρ ἡμᾶς·\n","  δεῦτε οὖν κατασοφισώμεθα αὐτούς, μή ποτε πληθυνθῇ, καὶ ἡνίκα ἂν συμβῇ ἡμῖν πόλεμος,\n","  προστεθήσονται καὶ οὗτοι πρὸς τοὺς ὑπεναντίους καὶ ἐκπολεμήσαντες ἡμᾶς ἐξελεύσονται ἐκ τῆς γῆς.\n","  καὶ ἐπέστησεν αὐτοῖς ἐπιστάτας τῶν ἔργων, ἵνα κακώσωσιν αὐτοὺς ἐν τοῖς ἔργοις· καὶ Ισραήλᾠκοδόμησαν πόλεις ὀχυρὰς τῷ Φαραώ, τήν τε Πειθὼ καὶ Ῥαμεσσῆ καὶ ῎Ων, ἥ ἐστιν ῾Ηλιούπολις.\n","  καθότι δὲ αὐτοὺς ἐταπείνουν, τοσούτῳ πλείους ἐγίγνοντο, καὶ ἴσχυον σφόδρα σφόδρα· καὶ ἐβδελύσσοντο οἱ Αἰγύπτιοι ἀπὸ τῶν υἱῶν ᾿.\n","  καὶ κατεδυνάστευον οἱ Αἰγύπτιοι τοὺς υἱοὺς ᾿Ισραὴλ βίᾳ καὶ κατωδύνων αὐτῶν τὴν ζωὴν ἐν τοῖς ἔργοις τοῖς σκληροῖς, τῷ πηλῷ καὶ τῇ πλινθείᾳ καὶ πᾶσι τοῖς ἔργοις τοῖς ἐν τοῖς πεδίοις, κατὰ πάντα τὰ ἔργα, ὧν κατεδουλοῦντο αὐτοὺς μετὰ βίας.\n","\"\"\")\n","\n","merged_results = []\n","\n","for r in ner(txt):\n","    if r['word'].startswith(\"##\"):\n","        merged_results[-1]['word'] += r['word'][2:]  # remove ## and join the subwords together instead of splitting it\n","        merged_results[-1]['score'] = max(merged_results[-1]['score'], r['score'])\n","    else:\n","        merged_results.append(r)\n","\n","for r in merged_results:\n","    print(f\"{r['word']:<20} → {r['entity_group']:<6} ({r['score']:.3f})\")\n"],"metadata":{"id":"expUuoIklEdc","executionInfo":{"status":"ok","timestamp":1763448753994,"user_tz":480,"elapsed":628,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"07289add-aec4-4a9e-a337-432a814c9286"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["αιγυπτον             → LOC    (0.991)\n","φαραω                → PERS   (0.998)\n","ραμεσση              → PERS   (0.837)\n","αιγυπτιοι            → GRP    (0.999)\n","αιγυπτιοι            → GRP    (0.999)\n"]}]},{"cell_type":"code","source":["FINAL_MODEL_DIR = OUTPUT_DIR\n","tokenizer_test = AutoTokenizer.from_pretrained(FINAL_MODEL_DIR)\n","model_test = AutoModelForTokenClassification.from_pretrained(FINAL_MODEL_DIR)\n","\n","trainer_test = Trainer(\n","    model=model_test,\n","    args=TrainingArguments(\n","        output_dir=\"./temp_eval\",\n","        per_device_eval_batch_size=32,\n","    ),\n","    eval_dataset=tokenised['test'], #used the Test dataset that was previously processed in same manner as the Train and Val\n","    tokenizer=tokenizer_test,\n","    data_collator=DataCollatorForTokenClassification(tokenizer_test),\n","    compute_metrics=compute_metrics,\n",")\n","\n","print(\"Running official test set evaluation...\")\n","results = trainer.evaluate()\n","\n","print(\"\\n\" + \"═\" * 60)\n","print(\"FINAL OFFICIAL TEST RESULTS (same as paper)\")\n","print(\"═\" * 60)\n","print(f\"Precision : {results['eval_precision']:.4f}\")\n","print(f\"Recall    : {results['eval_recall']:.4f}\")\n","print(f\"Micro F1  : {results['eval_f1']:.4f}\")\n","print(\"═\" * 60)\n","\n","if results['eval_f1'] > 0.826:\n","    print(\"We did better than the paper's 0.826!\")\n","else:\n","    print(\"Close to or matches the original paper result.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":271},"id":"WK-oZG9Lv3Tv","executionInfo":{"status":"ok","timestamp":1763449471910,"user_tz":480,"elapsed":12921,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}},"outputId":"0fd7929f-bb71-483b-f56a-822d5c2936a0"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-706216770.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer_test = Trainer(\n"]},{"output_type":"stream","name":"stdout","text":["Running official test set evaluation...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='278' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [139/139 01:00]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","════════════════════════════════════════════════════════════\n","FINAL OFFICIAL TEST RESULTS (same as paper)\n","════════════════════════════════════════════════════════════\n","Precision : 0.8327\n","Recall    : 0.8532\n","Micro F1  : 0.8428\n","════════════════════════════════════════════════════════════\n","We did better than the paper's 0.826!\n"]}]},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# Hyper-parameters\n","# ------------------------------------------------------------\n","LEARNING_RATE = 3e-5\n","BATCH_SIZE    = 32\n","EPOCHS        = 5\n","WEIGHT_DECAY  = 0.01\n","WARMUP_RATIO  = 1.0\n","SEED          = 123\n","OUTPUT_DIR    = f\"./tuned_ner_model_lr{LEARNING_RATE}_bs{BATCH_SIZE}_ep{EPOCHS}\"\n","\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    learning_rate=LEARNING_RATE,\n","    per_device_train_batch_size=BATCH_SIZE,\n","    per_device_eval_batch_size=BATCH_SIZE,\n","    num_train_epochs=EPOCHS,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_ratio=WARMUP_RATIO,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"f1\",\n","    greater_is_better=True,\n","    seed=SEED,\n","    logging_steps=10,\n","    save_total_limit=2,\n","    report_to=[],\n",")\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenised[\"train\"],\n","    eval_dataset=tokenised[\"validation\"],\n","    tokenizer=tokenizer,\n","    data_collator=collator,\n","    compute_metrics=compute_metrics,\n",")\n","\n","trainer.train()"],"metadata":{"id":"kyHhVrMr_yxg","executionInfo":{"status":"aborted","timestamp":1763432314554,"user_tz":480,"elapsed":34530,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"S58mPiWYeCkV","executionInfo":{"status":"aborted","timestamp":1763432314557,"user_tz":480,"elapsed":34533,"user":{"displayName":"Sarah Batara","userId":"04777559678768349971"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reference:\n","Beersmans, M., Keersmaekers, A., de Graaf, E., Van de Cruys, T., Depauw, M., & Fantoli, M. (2024). “Gotta catch ’em all!”: Retrieving people in Ancient Greek texts combining transformer models and domain knowledge. In J. Pavlopoulos et al. (Eds.), Proceedings of the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024) (pp. 152–164). Association for Computational Linguistics. https://doi.org/10.18653/v1/2024.ml4al-1.16"],"metadata":{"id":"fhkjhaBfmWgL"}}]}